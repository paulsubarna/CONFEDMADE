{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJANkmenWsaF"
      },
      "source": [
        "Mandatory imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-IjSfHlnWl6h"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-24 15:05:43.402930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-02-24 15:05:43.402960: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "from keras import backend as k\n",
        "from keras import metrics\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.layers import Input, Layer\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X_fuV-67BRVZ"
      },
      "outputs": [],
      "source": [
        "def random_shuffle(seed, _list):\n",
        "    random.seed(seed)\n",
        "    random.shuffle(_list)\n",
        "\n",
        "def np_save(base_dir, filename, data):\n",
        "  if os.path.isdir(base_dir) == False:\n",
        "      os.makedirs(base_dir)\n",
        "  np.save(os.path.join(base_dir, filename), data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWllsz-OWzcC"
      },
      "source": [
        "Load Dataset ( https://github.com/mgermain/MADE/releases/download/ICML2015/binarized_mnist.npz )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSfsHYFz9N5V",
        "outputId": "3a420b15-491a-4689-b84d-c6b3b8f09c79"
      },
      "outputs": [],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "x_temp = X_train\n",
        "x = x_temp.reshape(x_temp.shape[0],x_temp.shape[1]*x_temp.shape[2]) #flatten 28x28 pixels to one dimension (784 inputs)\n",
        "x = np.where(x > 127, 1, 0) #binarize x\n",
        "y = Y_train\n",
        "seed= 77\n",
        "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2]) #flatten 28x28 pixels to one dimension (784 inputs)\n",
        "X_test = np.where(X_test > 127, 1, 0) \n",
        "\n",
        "X_train= x[:50000]\n",
        "X_valid= x[50000:]\n",
        "\n",
        "base_dir = 'content'\n",
        "#for i,labels in enumerate(4):\n",
        "for i in range(10):\n",
        "  #idx= np.concatenate([np.where(y[:] == c)[0] for c in labels], axis=0)\n",
        "  idx_shuffled = np.arange(len(X_train))\n",
        "  random_shuffle(seed+ i*100, idx_shuffled)\n",
        "  X_train = X_train[idx_shuffled]\n",
        "  data= X_train[5000*i:5000*(i+1)]\n",
        "  filename= f'Task_{i}'\n",
        "  np_save(base_dir, filename, data)\n",
        "  #np.save(os.path.join(base_dir, filename), data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1N06S0jXBMe"
      },
      "source": [
        "Create Mask Generator Module for creating/managing MADEs masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DmT3Yp76XBtp"
      },
      "outputs": [],
      "source": [
        "class MaskGenerator(object):\n",
        "  # num_masks: The amount of masks that will be cycled through during training. if num_masks == 1 then connectivity agnostic training is disabled\n",
        "  # units_per_layer = Array containing # of units per layer\n",
        "  # seed = The seed used for randomly sampling the masks, for guaranteeing reproducability\n",
        "  # natural_input_order = Boolean defining if the natural input order (x1, x2, x3 etc) should be used\n",
        "  # current_mask: Integer to keep track of the mask currently used (xth mask)\n",
        "  # m: The mask values assigned to the networks units. 0 is the index of the input layer, 1 is the index of the first hidden layer and so on\n",
        "  def __init__(self, num_masks, units_per_layer, natural_input_order = False, seed=42):\n",
        "    self.num_masks = num_masks\n",
        "    self.units_per_layer = units_per_layer\n",
        "    self.seed = seed\n",
        "    self.natural_input_order = natural_input_order\n",
        "    self.current_mask = 0\n",
        "    self.m = {}\n",
        "\n",
        "    if natural_input_order: # init input ordering according to settings\n",
        "      self.m[0] = np.arange(self.units_per_layer[0])\n",
        "    else:\n",
        "      self.shuffle_inputs(return_mask = False)\n",
        "  \n",
        "  #Iterate through the hidden layers, resample new connectivity values m and build/return the resulting new masks\n",
        "  def shuffle_masks(self):\n",
        "    layer_amount = len(self.units_per_layer)\n",
        "    rng = np.random.RandomState(self.seed+self.current_mask)\n",
        "    self.current_mask = (self.current_mask + 1) % self.num_masks # Cycle through masks\n",
        "    for i in range(1, layer_amount -1): #skip input layer & output layer and only iterate through hidden_layers\n",
        "      self.m[i] = rng.randint(self.m[i-1].min(), self.units_per_layer[0] -1, size = self.units_per_layer[i]) # sample m from [min_m(previous_layer, d-1)] for all hidden units\n",
        "    new_masks = [tf.convert_to_tensor((self.m[l-1][:, None] <= self.m[l][None,:]), dtype=np.float32) for l in range(1, layer_amount-1)] # build hidden layer masks\n",
        "    new_masks.append(tf.convert_to_tensor((self.m[layer_amount-2][:, None] < self.m[0][None, :]), dtype = np.float32)) #build output layer mask. Note that the m values for the output layer are the same as for the input layer \n",
        "    return new_masks\n",
        "\n",
        "  # builds & returns direct mask. Call this method after shuffling inputs if order_agnostic training is active.\n",
        "  # Note that the Mask values m are the same for both input and output layers\n",
        "  def get_direct_mask(self):\n",
        "    return tf.convert_to_tensor((self.m[0][:, None] < self.m[0][None, :]), dtype = np.float32)\n",
        "\n",
        "  # shuffle input ordering and return new mask for first hidden layer\n",
        "  def shuffle_inputs(self, return_mask = True):\n",
        "    self.m[0] = np.random.permutation(self.units_per_layer[0])\n",
        "    if return_mask:\n",
        "      return tf.convert_to_tensor((self.m[0][:, None] <= self.m[1][None,:]), dtype=np.float32)\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk-sv9C9XDtt"
      },
      "source": [
        "Custom Layer for MADE masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lm1c8achXFmr"
      },
      "outputs": [],
      "source": [
        "# should be self explaining\n",
        "class MaskedLayer(Layer):\n",
        "    def __init__(self,\n",
        "                units,\n",
        "                mask,\n",
        "                activation='relu',\n",
        "                kernel_initializer='glorot_uniform',\n",
        "                bias_initializer='zeros',\n",
        "                **kwargs):\n",
        "      self.units = units\n",
        "      self.mask = mask\n",
        "      self.activation = activations.get(activation)\n",
        "      self.kernel_initializer = kernel_initializer\n",
        "      self.bias_initializer = bias_initializer\n",
        "      super(MaskedLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      #self.input_dim = input_shape[-1] if self.x_dim is None else input_shape[0][-1]\n",
        "\n",
        "      self.W = self.add_weight(shape=self.mask.shape,\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  name='W')\n",
        "\n",
        "      self.bias = self.add_weight(shape=(self.units,),\n",
        "                                      initializer=self.bias_initializer,\n",
        "                                      name='bias')\n",
        "\n",
        "      self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        ## Modified keras.Dense to account for the mask\n",
        "        masked_weights = self.W*self.mask\n",
        "        output = k.dot(inputs, masked_weights)\n",
        "        output = k.bias_add(output, self.bias, data_format = 'channels_last')\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def set_mask(self, mask):\n",
        "        self.mask = mask\n",
        "\n",
        "    def get_mask(self):\n",
        "        return self.mask\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ##Same as keras.Dense\n",
        "        assert input_shape and len(input_shape) >= 2\n",
        "        assert input_shape[-1]\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] = self.units\n",
        "        return tuple(output_shape)\n",
        "\n",
        "\n",
        "\n",
        "class ConditionningMaskedLayer(MaskedLayer):\n",
        "    def __init__(self, \n",
        "                units,\n",
        "                mask,\n",
        "                activation='relu',\n",
        "                kernel_initializer='glorot_uniform',\n",
        "                bias_initializer='zeros',\n",
        "                use_cond_mask=False,\n",
        "                **kwargs):\n",
        "        self.use_cond_mask = use_cond_mask\n",
        "        super(ConditionningMaskedLayer, self).__init__(units,\n",
        "                mask,\n",
        "                activation,\n",
        "                kernel_initializer,\n",
        "                bias_initializer, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.use_cond_mask:\n",
        "            self.U = self.add_weight(shape=self.mask.shape,\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='U')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.use_cond_mask == False:\n",
        "          return super().call(inputs)\n",
        "        masked_w_weights = self.W*self.mask\n",
        "        masked_u_weights_times_one_vec = k.dot(tf.ones(tf.shape(inputs)),self.U*self.mask)\n",
        "        weighted_input = k.dot(inputs, masked_w_weights)\n",
        "        weighted_input_and_bias = k.bias_add(weighted_input, self.bias, data_format = 'channels_last')\n",
        "        output = weighted_input_and_bias + masked_u_weights_times_one_vec\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class DirectInputConnectConditionningMaskedLayer(ConditionningMaskedLayer):\n",
        "      def __init__(self,\n",
        "                   units,\n",
        "                   mask,\n",
        "                   activation='relu',\n",
        "                   kernel_initializer='glorot_uniform',\n",
        "                   bias_initializer='zeros',\n",
        "                   use_cond_mask=False,\n",
        "                   direct_mask = None,\n",
        "                **kwargs):\n",
        "        self.direct_mask = direct_mask\n",
        "        super(DirectInputConnectConditionningMaskedLayer, self).__init__(units,\n",
        "                mask,\n",
        "                activation,\n",
        "                kernel_initializer,\n",
        "                bias_initializer,\n",
        "                use_cond_mask,\n",
        "                **kwargs)\n",
        "\n",
        "      def build(self, input_shape):\n",
        "        if self.direct_mask is not None:\n",
        "          self.D = self.add_weight(shape=self.direct_mask.shape,\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  name='D')\n",
        "        super().build(input_shape)\n",
        "\n",
        "      def set_mask(self, mask, direct = False):\n",
        "        if direct:\n",
        "          self.direct_mask = mask\n",
        "        else:\n",
        "          super().set_mask(mask)\n",
        "\n",
        "      def get_mask(self, direct = False):\n",
        "        if direct:\n",
        "          return self.direct_mask\n",
        "        else:\n",
        "          return super().get_mask\n",
        "\n",
        "      def call(self, inputs):\n",
        "        if self.direct_mask is None:\n",
        "          return super().call(inputs)\n",
        "        input, direct_input = inputs[0], inputs[1]\n",
        "\n",
        "        masked_w_weights = self.W*self.mask\n",
        "        weighted_input = k.dot(input, masked_w_weights)\n",
        "        weighted_input_and_bias = k.bias_add(weighted_input, self.bias, data_format = 'channels_last')\n",
        "        weighted_direct_input = k.dot(direct_input, self.D * self.direct_mask)\n",
        "\n",
        "        if self.use_cond_mask:\n",
        "          masked_u_weights_times_one_vec = k.dot(tf.ones(tf.shape(input)),self.U*self.mask)\n",
        "          output = weighted_direct_input + weighted_input_and_bias + masked_u_weights_times_one_vec\n",
        "\n",
        "        else: output = weighted_direct_input + weighted_input_and_bias\n",
        "\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj3sQmCCXHjj"
      },
      "source": [
        "# MADE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "leKoxMM1XJtn"
      },
      "outputs": [],
      "source": [
        "# outputs: output Layer   ---------- Both needed when using ----------\n",
        "# inputs: input Layer     ----------    base keras.Model    ----------     \n",
        "# mask_generator: Mask Generator instance that manages the Models Masks\n",
        "# order_agn: Boolean defining if training should be order_agnostic\n",
        "# conn_agn: Boolean defining if training should be connectivity_agnostic\n",
        "# direct_input: Boolean defining if direct input masks should be used\n",
        "class ModelMADE(tf.keras.Model):\n",
        "    def __init__(self, inputs, outputs, mask_generator, order_agn, conn_agn,\n",
        "                 direct_input, **kwargs):\n",
        "      super(ModelMADE, self).__init__(inputs = inputs, outputs = outputs, **kwargs)\n",
        "      self.mask_generator = mask_generator\n",
        "      self.order_agn = order_agn\n",
        "      self.conn_agn = conn_agn\n",
        "      self.direct_input = direct_input\n",
        "    \n",
        "    # Method called by fit for every batch\n",
        "    def train_step(self, data):\n",
        "\n",
        "      # reoder inputs, change masks\n",
        "      if self.order_agn:\n",
        "        # order agnostic and connectivity agnostic training\n",
        "        if self.conn_agn:\n",
        "          self.mask_generator.shuffle_inputs(return_mask = False)\n",
        "          new_masks = self.mask_generator.shuffle_masks()\n",
        "          for hidden_layer_id in range(len(new_masks)):\n",
        "            self.layers[1+hidden_layer_id].set_mask(new_masks[hidden_layer_id]) #assign layer+1 since the first layer is no hidden layer and has no mask\n",
        "        \n",
        "        # order agnostic but not connectivity agnostic training        \n",
        "        else:\n",
        "          self.layers[1].set_mask(self.mask_generator.shuffle_inputs())\n",
        "        if self.direct_input:\n",
        "          self.layers[-1].set_mask(self.mask_generator.get_direct_mask(), direct=True)\n",
        "\n",
        "      # not order agnostic but connectivity agnostic training\n",
        "      elif self.conn_agn:\n",
        "        new_masks = self.mask_generator.shuffle_masks()\n",
        "        for hidden_layer_id in range(len(new_masks)):\n",
        "          self.layers[1+hidden_layer_id].set_mask(new_masks[hidden_layer_id])\n",
        "\n",
        "\n",
        "      # Unpack the data. Its structure depends on your model and\n",
        "      # on what you pass to `fit()`.\n",
        "      x, y = data\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_pred = self(x, training=True)  # Forward pass\n",
        "        # Compute the loss value\n",
        "        # (the loss function is configured in `compile()`)\n",
        "        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "\n",
        "      # Compute gradients\n",
        "      trainable_vars = self.trainable_variables\n",
        "      gradients = tape.gradient(loss, trainable_vars)\n",
        "      # Update weights\n",
        "      self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "      # Update metrics (includes the metric that tracks the loss)\n",
        "      self.compiled_metrics.update_state(y, y_pred)\n",
        "      # Return a dict mapping metric names to current value\n",
        "      return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af_jZIw7XLfA"
      },
      "source": [
        "# MADE Object\n",
        "responsible for building and inintalizing the MADE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6v-oRKzRXNnf"
      },
      "outputs": [],
      "source": [
        "# units_per_layer = Array containing # of units per layer\n",
        "# natural_input_order = Boolean defining if the natural input order (x1, x2, x3 etc) should be used\n",
        "# num_masks: The amount of masks that will be cycled through during training. if num_masks == 1 then connectivity agnostic training is disabled\n",
        "# order_agn: Boolean defining if training should be order_agnostic\n",
        "# connectivity_weights: Boolean defining if connectivity weights should be used\n",
        "# direct input: Boolean defining if there should be a direct input connection between input & output layer\n",
        "  # seed = The seed used for randomly sampling the masks, for guaranteeing reproducability\n",
        "class MADE(object):\n",
        "  def __init__(self, units_per_layer, natural_input_order, num_masks, order_agn,\n",
        "               connectivity_weights, direct_input, seed = \"42\"):\n",
        "    self.units_per_layer = units_per_layer\n",
        "    self.natural_input_order = natural_input_order\n",
        "    self.num_masks = num_masks\n",
        "    self.order_agn = order_agn\n",
        "    self.connectivity_weights = connectivity_weights\n",
        "    self.direct_input = direct_input\n",
        "    self.seed = seed\n",
        "    self.mask_generator = MaskGenerator(num_masks, units_per_layer, natural_input_order, seed)\n",
        "\n",
        "  def build_model(self):\n",
        "    # build input layer\n",
        "    a = Input(shape = (self.units_per_layer[0],))\n",
        "    x_layers = []\n",
        "      \n",
        "    #build masks\n",
        "    masks = self.mask_generator.shuffle_masks()\n",
        "    direct_mask = None\n",
        "\n",
        "    #build hidden layers  \n",
        "    for i in range(1,len(self.units_per_layer)-1): #exclude input & output layer\n",
        "      if i == 1:\n",
        "        x_layers.append(ConditionningMaskedLayer(units = self.units_per_layer[i], mask = masks[i-1], use_cond_mask = self.connectivity_weights)(a)) #activation is relu, call custom_masking with previous layer as input-param\n",
        "      else:\n",
        "        x_layers.append(ConditionningMaskedLayer(units = self.units_per_layer[i], mask = masks[i-1], use_cond_mask = self.connectivity_weights)(x_layers[i-1]))\n",
        "          \n",
        "    #build output layer, output layer's activation is sigmoid.\n",
        "    if self.direct_input:\n",
        "      direct_mask = self.mask_generator.get_direct_mask()\n",
        "      output_layer = DirectInputConnectConditionningMaskedLayer(units = self.units_per_layer[-1], mask = masks[-1], activation='sigmoid', use_cond_mask = self.connectivity_weights, direct_mask = direct_mask)([x_layers[-1], a])\n",
        "    else:\n",
        "      output_layer = ConditionningMaskedLayer(units = self.units_per_layer[-1], mask = masks[-1], activation='sigmoid', use_cond_mask = self.connectivity_weights)(x_layers[-1])\n",
        "    x_layers.append(output_layer)\n",
        "    \n",
        "    self.model = ModelMADE(inputs = a, outputs = x_layers[-1], mask_generator = self.mask_generator, order_agn = self.order_agn, conn_agn = self.num_masks>1,\n",
        "                           direct_input=self.direct_input)\n",
        "    return self.model\n",
        "\n",
        "  def summary(self):\n",
        "    return self.model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot0YX5q0XRGP"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4XpIHE6eXRw2"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(x, x_decoded_mean):\n",
        "    x = k.flatten(x)\n",
        "    x_decoded_mean = k.flatten(x_decoded_mean)\n",
        "    xent_loss = 784 * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "    #print(\"shape\", len(X_train[1]))\n",
        "    return xent_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ojmCu2d043Sa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def average_model(client_weights):\n",
        "  av_param= []\n",
        "  avg= 1/len(client_weights)\n",
        "  #avg= 0.1\n",
        "  print(avg)\n",
        "  for i in range(len(client_weights[0])):\n",
        "    av_param.append([])\n",
        "  for i in range(len(client_weights)):\n",
        "    for j in range(len(client_weights[i])):\n",
        "      if i ==0:\n",
        "        av_param[j].append(avg * client_weights[i][j])\n",
        "      else:\n",
        "        av_param[j] = av_param[j] + (avg * client_weights[i][j])\n",
        "  for i in range(len(client_weights[0])):\n",
        "    av_param[i]=np.squeeze(av_param[i])\n",
        "  return av_param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqgBcy8XU3U"
      },
      "source": [
        "# Build & Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq3iJXffXVcG",
        "outputId": "1dea136e-2f1e-4f32-b7f7-bf3c9e21c52d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape [784 500 784]\n",
            "Model: \"model_made\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 784)]        0           []                               \n",
            "                                                                                                  \n",
            " conditionning_masked_layer (Co  (None, 500)         392500      ['input_1[0][0]']                \n",
            " nditionningMaskedLayer)                                                                          \n",
            "                                                                                                  \n",
            " direct_input_connect_condition  (None, 784)         1007440     ['conditionning_masked_layer[0][0\n",
            " ning_masked_layer (DirectInput                                  ]',                              \n",
            " ConnectConditionningMaskedLaye                                   'input_1[0][0]']                \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,399,940\n",
            "Trainable params: 1,399,940\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "enter\n",
            "50/50 [==============================] - 1s 22ms/step - loss: 328.2405\n",
            "Client0\n",
            "50/50 [==============================] - 1s 22ms/step - loss: 236.6115\n",
            "Client1\n",
            "50/50 [==============================] - 1s 22ms/step - loss: 209.7887\n",
            "Client2\n",
            "50/50 [==============================] - 1s 22ms/step - loss: 190.7898\n",
            "Client3\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 179.6448\n",
            "Client4\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 170.4259\n",
            "Client5\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 164.0221\n",
            "Client6\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 157.2082\n",
            "Client7\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 153.6347\n",
            "Client8\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 150.2250\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 172.9598\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 165.9368\n",
            "Client1\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 160.7124\n",
            "Client2\n",
            "50/50 [==============================] - 1s 23ms/step - loss: 154.9939\n",
            "Client3\n",
            "50/50 [==============================] - 1s 22ms/step - loss: 152.0293\n",
            "Client4\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 146.8603\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 144.9884\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 141.3622\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 140.4599\n",
            "Client8\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 138.5236\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 147.7659\n",
            "Client0\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 144.6271\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 141.5518\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 139.0370\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 137.4824\n",
            "Client4\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 134.4489\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 133.5305\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 131.7410\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 131.0101\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 130.3456\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 134.9159\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 133.2357\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 131.8273\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 129.0368\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 128.9805\n",
            "Client4\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 126.4703\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 125.7249\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 124.6995\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 124.0361\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 123.6564\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 126.8226\n",
            "Client0\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 125.2764\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 124.6712\n",
            "Client2\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 123.1489\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 122.4052\n",
            "Client4\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 120.5767\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 120.7051\n",
            "Client6\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 119.4892\n",
            "Client7\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 119.9145\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 119.0069\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 121.0690\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 119.9372\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 119.2488\n",
            "Client2\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 117.4261\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 116.9609\n",
            "Client4\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 116.3042\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 115.7971\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 115.2959\n",
            "Client7\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 115.2596\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 114.6475\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 116.7715\n",
            "Client0\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 115.7245\n",
            "Client1\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 114.7989\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 113.4208\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 113.5903\n",
            "Client4\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 112.3074\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 112.6905\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 111.1311\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 111.8960\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 111.4347\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 112.8015\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 111.6959\n",
            "Client1\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 111.1574\n",
            "Client2\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 110.0107\n",
            "Client3\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 110.2191\n",
            "Client4\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 109.4329\n",
            "Client5\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 109.1821\n",
            "Client6\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 108.0664\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 108.8542\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 108.3926\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 109.7033\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 108.9445\n",
            "Client1\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 107.9452\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 107.7127\n",
            "Client3\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 107.8186\n",
            "Client4\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 106.9696\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 106.9198\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 106.1296\n",
            "Client7\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 106.8664\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 106.2836\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 107.2114\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 106.2400\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 106.4265\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 105.4638\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 105.3682\n",
            "Client4\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 104.2250\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 104.9001\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 103.7938\n",
            "Client7\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 104.5612\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 104.2482\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 105.0091\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 104.2055\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 103.9347\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 102.8174\n",
            "Client3\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 102.8363\n",
            "Client4\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 102.6961\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 102.0658\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 102.0712\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 102.5712\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 102.2263\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 102.8404\n",
            "Client0\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 101.9644\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 102.1331\n",
            "Client2\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 101.1059\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 100.8605\n",
            "Client4\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 100.9274\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 100.6946\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 100.2934\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 101.2223\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 100.9697\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 100.6678\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 100.4111\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 100.5875\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 99.4974\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 100.1040\n",
            "Client4\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 98.7963\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 99.2509\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 98.9311\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 99.6083\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 98.9672\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 99.6901\n",
            "Client0\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 99.1643\n",
            "Client1\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 98.7763\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 97.9062\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 98.0511\n",
            "Client4\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 97.4105\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 97.9027\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 97.8176\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 98.5032\n",
            "Client8\n",
            "50/50 [==============================] - 1s 23ms/step - loss: 98.1118\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 97.8515\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 97.6896\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 97.4616\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 96.4607\n",
            "Client3\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 96.3752\n",
            "Client4\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 96.6081\n",
            "Client5\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 96.7794\n",
            "Client6\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 96.1176\n",
            "Client7\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 97.0180\n",
            "Client8\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 97.0246\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 96.6882\n",
            "Client0\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 95.9397\n",
            "Client1\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 96.3158\n",
            "Client2\n",
            "50/50 [==============================] - 1s 19ms/step - loss: 95.2904\n",
            "Client3\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 95.7025\n",
            "Client4\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 94.7628\n",
            "Client5\n",
            "50/50 [==============================] - 1s 22ms/step - loss: 95.6958\n",
            "Client6\n",
            "50/50 [==============================] - 1s 23ms/step - loss: 95.3404\n",
            "Client7\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 95.9459\n",
            "Client8\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 95.8429\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 95.5883\n",
            "Client0\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 95.1520\n",
            "Client1\n",
            "50/50 [==============================] - 1s 23ms/step - loss: 95.3253\n",
            "Client2\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 94.3526\n",
            "Client3\n",
            "50/50 [==============================] - 1s 23ms/step - loss: 94.7334\n",
            "Client4\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 94.0462\n",
            "Client5\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 93.7348\n",
            "Client6\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 93.9198\n",
            "Client7\n",
            "50/50 [==============================] - 1s 20ms/step - loss: 94.8219\n",
            "Client8\n",
            "50/50 [==============================] - 1s 23ms/step - loss: 94.1221\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 94.6941\n",
            "Client0\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 94.2730\n",
            "Client1\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 94.0795\n",
            "Client2\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 93.5211\n",
            "Client3\n",
            "50/50 [==============================] - 1s 21ms/step - loss: 93.6600\n",
            "Client4\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 93.2280\n",
            "Client5\n",
            "50/50 [==============================] - 2s 37ms/step - loss: 92.8631\n",
            "Client6\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 92.8975\n",
            "Client7\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 93.9606\n",
            "Client8\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 93.4754\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 93.8739\n",
            "Client0\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 93.1154\n",
            "Client1\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 93.5669\n",
            "Client2\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.5775\n",
            "Client3\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.8640\n",
            "Client4\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.9897\n",
            "Client5\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.3794\n",
            "Client6\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 92.0246\n",
            "Client7\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 93.1815\n",
            "Client8\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 92.9221\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.9818\n",
            "Client0\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.6659\n",
            "Client1\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.5425\n",
            "Client2\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 92.1719\n",
            "Client3\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.1706\n",
            "Client4\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 91.7288\n",
            "Client5\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 91.6845\n",
            "Client6\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 91.7754\n",
            "Client7\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 92.2259\n",
            "Client8\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 92.1144\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 92.2211\n",
            "Client0\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 91.2475\n",
            "Client1\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 91.8401\n",
            "Client2\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 90.7345\n",
            "Client3\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 91.5042\n",
            "Client4\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 90.8873\n",
            "Client5\n",
            "50/50 [==============================] - 2s 32ms/step - loss: 90.9622\n",
            "Client6\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 91.2885\n",
            "Client7\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 91.6816\n",
            "Client8\n",
            "50/50 [==============================] - 2s 32ms/step - loss: 91.0656\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 90.9650\n",
            "Client0\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 91.3292\n",
            "Client1\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 90.9341\n",
            "Client2\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 90.5202\n",
            "Client3\n",
            "50/50 [==============================] - 2s 32ms/step - loss: 91.1707\n",
            "Client4\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 90.0176\n",
            "Client5\n",
            "50/50 [==============================] - 2s 37ms/step - loss: 90.5536\n",
            "Client6\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 90.4735\n",
            "Client7\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 91.1555\n",
            "Client8\n",
            "50/50 [==============================] - 2s 37ms/step - loss: 90.7964\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 2s 33ms/step - loss: 90.9760\n",
            "Client0\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 90.5005\n",
            "Client1\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 90.0534\n",
            "Client2\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 89.7737\n",
            "Client3\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 89.2285\n",
            "Client4\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 88.9873\n",
            "Client5\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 89.5088\n",
            "Client6\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 89.0780\n",
            "Client7\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 89.7343\n",
            "Client8\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 90.0606\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 90.3961\n",
            "Client0\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 89.9306\n",
            "Client1\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 89.3377\n",
            "Client2\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 89.0478\n",
            "Client3\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 88.4424\n",
            "Client4\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 88.4023\n",
            "Client5\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 88.3110\n",
            "Client6\n",
            "50/50 [==============================] - 2s 36ms/step - loss: 88.7441\n",
            "Client7\n",
            "50/50 [==============================] - 2s 32ms/step - loss: 89.7307\n",
            "Client8\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 89.9477\n",
            "Client9\n",
            "0.1\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 89.7995\n",
            "Client0\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 88.9540\n",
            "Client1\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 88.9416\n",
            "Client2\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 87.7855\n",
            "Client3\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 88.8973\n",
            "Client4\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 88.1828\n",
            "Client5\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 88.2964\n",
            "Client6\n",
            "50/50 [==============================] - 1s 25ms/step - loss: 88.3562\n",
            "Client7\n",
            "50/50 [==============================] - 1s 24ms/step - loss: 88.9673\n",
            "Client8\n",
            "50/50 [==============================] - 1s 23ms/step - loss: 89.2750\n",
            "Client9\n",
            "0.1\n",
            "Elapsed:  291.4245128631592\n",
            "Number of masks: 1\n",
            " 11/100 [==>...........................] - ETA: 0s - loss: 89.0685"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-24 15:28:56.431101: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 62720000 exceeds 10% of free system memory.\n",
            "2023-02-24 15:28:56.442461: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 62720000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 1s 10ms/step - loss: 87.6962\n",
            "Test Loss: 87.69623565673828\n"
          ]
        }
      ],
      "source": [
        "######################### Settings #########################\n",
        "_optimizer_type = \"adam\" #for any other string here then adam Adagrad is used\n",
        "_adam_lr = 0.001 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_lr = 0.005 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_epsilon = 1e-6\n",
        "\n",
        "_hidden_layers = [500]\n",
        "_natural_input_order = True\n",
        "_num_masks = 1\n",
        "_order_agn = True\n",
        "_order_agn_step_size = 1\n",
        "_conn_agn_step_size = 1\n",
        "_connectivity_weights = False\n",
        "_direct_input = True\n",
        "_seed = 42\n",
        "_batch_size = 100\n",
        "_epochs = 1\n",
        "\n",
        "if _optimizer_type == \"ada\": \n",
        "  optimizer = Adam(_adam_lr)\n",
        "else: \n",
        "  optimizer = Adagrad(_ada_lr, epsilon= _ada_epsilon)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "units_per_layer = np.concatenate(([784], _hidden_layers, [784])) #in MADE case the input & output layer have the same amount of units\n",
        "print(\"shape\",units_per_layer)\n",
        "\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed)\n",
        "model = temp.build_model()\n",
        "model.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "model.summary()\n",
        "re= model.get_weights()\n",
        "start = time.time()\n",
        "print(\"enter\")\n",
        "num_clients = 10\n",
        "num_rounds= 25\n",
        "num_tasks= 1\n",
        "loss = {}\n",
        "for c in range(num_clients):\n",
        "  loss[f'{c}']= []\n",
        "\n",
        "val_loss = {}\n",
        "for c in range(num_clients):\n",
        "  val_loss[f'{c}']= []\n",
        "for t in range(num_tasks):\n",
        "  for r in range(num_rounds):\n",
        "    client_weights= []\n",
        "    for c in range(num_clients):\n",
        "      X= np.load(f'content/Task_{t*num_clients+ c}.npy')\n",
        "      history = model.fit(\n",
        "          X, X,\n",
        "          batch_size=_batch_size,\n",
        "          epochs=_epochs\n",
        "          \n",
        "      )\n",
        "      \n",
        "      print(f'Client{c}')\n",
        "\n",
        "      loss[f'{c}'].append(history.history['loss'])\n",
        "      el= model.get_weights()\n",
        "      client_weights.append(el)\n",
        "    model_param= average_model(client_weights)\n",
        "    model.set_weights(model_param)\n",
        "\n",
        "done = time.time()\n",
        "elapsed = done - start\n",
        "print(\"Elapsed: \", elapsed)\n",
        "print(f\"Number of masks: {_num_masks}\")\n",
        "test_loss=model.evaluate(X_test, X_test, batch_size=_batch_size)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7_nsXIIWG3n"
      },
      "outputs": [],
      "source": [
        "np.save('/content/VAL_Loss_CM', val_loss)\n",
        "np.save('/content/Loss_CM', loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USOG4u8Ry-Ij",
        "outputId": "5ec2f609-b0b8-4d04-e91e-73408f6913c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ],
      "source": [
        "weights= model.get_weights()\n",
        "np.save('/content/modelweights', weights)\n",
        "from tensorflow import keras\n",
        "#model.save('/content/mymodel')\n",
        "#model= keras.models.load_model('/content/mymodel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ori8RPGOGzKk",
        "outputId": "c0c15a8c-b9d3-46e4-fba9-7ad2104501ab"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (3020081042.py, line 26)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Input \u001b[0;32mIn [17]\u001b[0;36m\u001b[0m\n\u001b[0;31m    for c in range(num_clients):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "######################### Settings #########################\n",
        "_optimizer_type = \"ada\" #for any other string here then adam Adagrad is used\n",
        "_adam_lr = 0.001 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_lr = 0.005 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_epsilon = 1e-6\n",
        "\n",
        "_hidden_layers = [500]\n",
        "_natural_input_order = True,\n",
        "_num_masks = 1\n",
        "_order_agn = False\n",
        "_order_agn_step_size = 1\n",
        "_conn_agn_step_size = 1\n",
        "_connectivity_weights = False\n",
        "_direct_input = False\n",
        "_seed = 42\n",
        "_batch_size = 100\n",
        "_epochs = 1\n",
        "\n",
        "if _optimizer_type == \"adam\": \n",
        "  optimizer = Adam(_adam_lr)\n",
        "else: \n",
        "  optimizer = Adagrad(_ada_lr, epsilon = _ada_epsilon)\n",
        "\n",
        "\n",
        "loss = {}\n",
        "  for c in range(num_clients):\n",
        "    loss[f'{c}']= []\n",
        "\n",
        "val_loss = {}\n",
        "  for c in range(num_clients):\n",
        "    val_loss[f'{c}']= []\n",
        "units_per_layer = np.concatenate(([784], _hidden_layers, [784])) #in MADE case the input & output layer have the same amount of units\n",
        "print(\"shape\",units_per_layer)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed)\n",
        "model1 = temp.build_model()\n",
        "model1.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 21)\n",
        "model2 = temp.build_model()\n",
        "model2.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 62)\n",
        "model3 = temp.build_model()\n",
        "model3.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 50)\n",
        "model4 = temp.build_model()\n",
        "model4.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 70)\n",
        "model5 = temp.build_model()\n",
        "model5.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 80)\n",
        "model6 = temp.build_model()\n",
        "model6.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 90)\n",
        "model7 = temp.build_model()\n",
        "model7.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 85)\n",
        "model8 = temp.build_model()\n",
        "model8.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 67)\n",
        "model9 = temp.build_model()\n",
        "model9.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 61)\n",
        "model10 = temp.build_model()\n",
        "model10.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+78)\n",
        "model11 = temp.build_model()\n",
        "model11.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 227)\n",
        "model12 = temp.build_model()\n",
        "model12.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 260)\n",
        "model13 = temp.build_model()\n",
        "model13.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 360)\n",
        "model14 = temp.build_model()\n",
        "model14.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 460)\n",
        "model15 = temp.build_model()\n",
        "model15.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 160)\n",
        "model16 = temp.build_model()\n",
        "model16.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 860)\n",
        "model17 = temp.build_model()\n",
        "model17.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 760)\n",
        "model18 = temp.build_model()\n",
        "model18.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 560)\n",
        "model19 = temp.build_model()\n",
        "model19.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 660)\n",
        "model20 = temp.build_model()\n",
        "model20.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 143)\n",
        "model21 = temp.build_model()\n",
        "model21.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 111)\n",
        "model22 = temp.build_model()\n",
        "model22.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 197)\n",
        "model23 = temp.build_model()\n",
        "model23.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 199)\n",
        "model24 = temp.build_model()\n",
        "model24.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 189)\n",
        "model25 = temp.build_model()\n",
        "model25.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 154)\n",
        "model26 = temp.build_model()\n",
        "model26.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 139)\n",
        "model27 = temp.build_model()\n",
        "model27.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 136)\n",
        "model28 = temp.build_model()\n",
        "model28.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 134)\n",
        "model29 = temp.build_model()\n",
        "model29.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 110)\n",
        "model30 = temp.build_model()\n",
        "model30.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 743)\n",
        "model31 = temp.build_model()\n",
        "model31.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 811)\n",
        "model32 = temp.build_model()\n",
        "model32.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 697)\n",
        "model33 = temp.build_model()\n",
        "model33.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 699)\n",
        "model34 = temp.build_model()\n",
        "model34.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 589)\n",
        "model35 = temp.build_model()\n",
        "model35.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 454)\n",
        "model36 = temp.build_model()\n",
        "model36.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 339)\n",
        "model37 = temp.build_model()\n",
        "model37.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 236)\n",
        "model38 = temp.build_model()\n",
        "model38.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 234)\n",
        "model39 = temp.build_model()\n",
        "model39.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed+ 210)\n",
        "model40 = temp.build_model()\n",
        "model40.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "#model.summary()\n",
        "\n",
        "start = time.time()\n",
        "print(\"enter\")\n",
        "num_clients = 40\n",
        "num_rounds= 25\n",
        "num_tasks= 1\n",
        "for t in range(num_tasks):\n",
        "  for r in range(num_rounds):\n",
        "    client_weights= []\n",
        "    for c in range(num_clients):\n",
        "      X= np.load(f'content/Task_{t*num_clients+ c}.npy')\n",
        "      if c == 0:\n",
        "        history = model1.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs,\n",
        "            validation_data=(X_valid, X_valid),\n",
        "        )\n",
        "\n",
        "        el= model1.get_weights()\n",
        "        client_weights.append(el)\n",
        "      elif c == 1:\n",
        "        history1 = model2.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs,\n",
        "            validation_data=(X_valid, X_valid),\n",
        "        )\n",
        "        val_loss[f'{c}'].append(history1.history['val_loss'])\n",
        "        loss[f'{c}'].append(history1.history['loss'])\n",
        "        el2= model2.get_weights()\n",
        "        client_weights.append(el2)\n",
        "      elif c ==2:\n",
        "        history2 = model3.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs,\n",
        "            validation_data=(X_valid, X_valid),\n",
        "        )\n",
        "        val_loss[f'{c}'].append(history2.history['val_loss'])\n",
        "        loss[f'{c}'].append(history2.history['loss'])\n",
        "        el3= model3.get_weights()\n",
        "        client_weights.append(el3)\n",
        "      elif c ==3:\n",
        "        history3 = model4.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history3.history['loss'])\n",
        "        el4= model4.get_weights()\n",
        "        client_weights.append(el4)\n",
        "      elif c ==4:\n",
        "        history4 = model5.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history4.history['loss'])\n",
        "        el5= model5.get_weights()\n",
        "        client_weights.append(el5)\n",
        "      elif c ==5:\n",
        "        history5 = model6.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history5.history['loss'])\n",
        "        el6= model6.get_weights()\n",
        "        client_weights.append(el6)\n",
        "      elif c ==6:\n",
        "        history6 = model7.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history6.history['loss'])\n",
        "        el7= model7.get_weights()\n",
        "        client_weights.append(el7)\n",
        "      elif c ==7:\n",
        "        history7 = model8.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history7.history['loss'])\n",
        "        el8= model8.get_weights()\n",
        "        client_weights.append(el8)\n",
        "      elif c ==8:\n",
        "        history8 = model9.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history8.history['loss'])\n",
        "        el9= model9.get_weights()\n",
        "        client_weights.append(el9)\n",
        "      elif c ==9:\n",
        "        history9 = model10.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history9.history['loss'])\n",
        "        el10= model10.get_weights()\n",
        "        client_weights.append(el10)\n",
        "            for c in range(num_clients):\n",
        "      X= np.load(f'content/Task_{t*num_clients+ c}.npy')\n",
        "      if c == 0:\n",
        "        history = model1.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs,\n",
        "            validation_data=(X_valid, X_valid),\n",
        "        )\n",
        "\n",
        "        el= model1.get_weights()\n",
        "        client_weights.append(el)\n",
        "      elif c == 1:\n",
        "        history1 = model2.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs,\n",
        "            validation_data=(X_valid, X_valid),\n",
        "        )\n",
        "        val_loss[f'{c}'].append(history1.history['val_loss'])\n",
        "        loss[f'{c}'].append(history1.history['loss'])\n",
        "        el2= model2.get_weights()\n",
        "        client_weights.append(el2)\n",
        "      elif c ==2:\n",
        "        history2 = model3.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs,\n",
        "            validation_data=(X_valid, X_valid),\n",
        "        )\n",
        "        val_loss[f'{c}'].append(history2.history['val_loss'])\n",
        "        loss[f'{c}'].append(history2.history['loss'])\n",
        "        el3= model3.get_weights()\n",
        "        client_weights.append(el3)\n",
        "      elif c ==3:\n",
        "        history3 = model4.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history3.history['loss'])\n",
        "        el4= model4.get_weights()\n",
        "        client_weights.append(el4)\n",
        "      elif c ==4:\n",
        "        history4 = model5.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history4.history['loss'])\n",
        "        el5= model5.get_weights()\n",
        "        client_weights.append(el5)\n",
        "      elif c ==5:\n",
        "        history5 = model6.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history5.history['loss'])\n",
        "        el6= model6.get_weights()\n",
        "        client_weights.append(el6)\n",
        "      elif c ==6:\n",
        "        history6 = model7.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history6.history['loss'])\n",
        "        el7= model7.get_weights()\n",
        "        client_weights.append(el7)\n",
        "      elif c ==7:\n",
        "        history7 = model8.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history7.history['loss'])\n",
        "        el8= model8.get_weights()\n",
        "        client_weights.append(el8)\n",
        "      elif c ==8:\n",
        "        history8 = model9.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history8.history['loss'])\n",
        "        el9= model9.get_weights()\n",
        "        client_weights.append(el9)\n",
        "      elif c ==9:\n",
        "        history9 = model10.fit(\n",
        "            X, X,\n",
        "            batch_size=_batch_size,\n",
        "            epochs=_epochs\n",
        "        )\n",
        "        #val_loss[f'{c}'].append(history3.history['val_loss'])\n",
        "        loss[f'{c}'].append(history9.history['loss'])\n",
        "        el10= model10.get_weights()\n",
        "        client_weights.append(el10)\n",
        "    model_param= average_model(client_weights)\n",
        "    model1.set_weights(model_param)\n",
        "    model2.set_weights(model_param)\n",
        "    model3.set_weights(model_param)\n",
        "    model4.set_weights(model_param)\n",
        "    model5.set_weights(model_param)\n",
        "    model6.set_weights(model_param)\n",
        "    model7.set_weights(model_param)\n",
        "    model8.set_weights(model_param)\n",
        "    model9.set_weights(model_param)\n",
        "    model10.set_weights(model_param)\n",
        "    model11.set_weights(model_param)\n",
        "    model12.set_weights(model_param)\n",
        "    model13.set_weights(model_param)\n",
        "    model14.set_weights(model_param)\n",
        "    model15.set_weights(model_param)\n",
        "    model16.set_weights(model_param)\n",
        "    model17.set_weights(model_param)\n",
        "    model18.set_weights(model_param)\n",
        "    model19.set_weights(model_param)\n",
        "    model20.set_weights(model_param)\n",
        "    model21.set_weights(model_param)\n",
        "    model22.set_weights(model_param)\n",
        "    model23.set_weights(model_param)\n",
        "    model24.set_weights(model_param)\n",
        "    model25.set_weights(model_param)\n",
        "    model26.set_weights(model_param)\n",
        "    model27.set_weights(model_param)\n",
        "    model28.set_weights(model_param)\n",
        "    model29.set_weights(model_param)\n",
        "    model30.set_weights(model_param)\n",
        "    model31.set_weights(model_param)\n",
        "    model32.set_weights(model_param)\n",
        "    model33.set_weights(model_param)\n",
        "    model34.set_weights(model_param)\n",
        "    model35.set_weights(model_param)\n",
        "    model36.set_weights(model_param)\n",
        "    model37.set_weights(model_param)\n",
        "    model38.set_weights(model_param)\n",
        "    model39.set_weights(model_param)\n",
        "    model40.set_weights(model_param)\n",
        "    tf.keras.backend.clear_session()\n",
        "done = time.time()\n",
        "elapsed = done - start\n",
        "print(\"Elapsed: \", elapsed)\n",
        "print(f\"Number of masks: {_num_masks}\")\n",
        "test_loss=model.evaluate(X_valid, X_valid, batch_size=_batch_size)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNJ4Gv1zHDwl"
      },
      "outputs": [],
      "source": [
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = 777)\n",
        "model1, masks = temp.build_model()\n",
        "temp1 = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = 222)\n",
        "model2, masks2 = temp1.build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsPE26WB4v7F"
      },
      "outputs": [],
      "source": [
        "np.save('/content/VAL_Loss_FM', val_loss)\n",
        "np.save('/content/Loss_FM', loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4JgTX7h9t-5"
      },
      "outputs": [],
      "source": [
        "arr= np.load('/content/VAL_Loss_FM.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5KLMR739xp9"
      },
      "outputs": [],
      "source": [
        "model2.set_weights(el)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqGySI9q98Xu",
        "outputId": "03429a97-86be-42fe-f9f2-efb3879e82ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array_equal(one[1],two[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nmVqbZM9-7X",
        "outputId": "55b276dd-f070-4960-9bb4-cf98f5382e5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array_equal(masks[1],masks2[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QtfLJRxg-CKi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 0 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "T = [12,10,8]\n",
        "index = T.index(max(T))\n",
        "\n",
        "T = np.abs(np.array(T)) * 0\n",
        "T[index] = 1\n",
        "print(T)\n",
        "#T = torch.FloatTensor(T)\n",
        "# Create the list of relevances with (L + 1) elements and assign the value of the last one \n",
        "#R = [None] * L + [(A[-1].cpu() * T).data + 1e-6]\n",
        "T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 4)\n",
            "(3, 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3, 4)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a=np.array([0, 1, 2])\n",
        "a=a[:,np.newaxis]\n",
        "b=np.array([[ 0,  1,  2,  3],\n",
        "              [ 4,  5,  6,  7],\n",
        "              [ 8,  9, 10, 11]])\n",
        "\n",
        "b=np.random.rand(3,4)\n",
        "        \n",
        "print(b.shape)\n",
        "print(a.shape)\n",
        "row_vc= (a*b)\n",
        "row_vc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
