{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulsubarna/CONFEDMADE/blob/main/tutorial_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f31f76",
      "metadata": {
        "id": "50f31f76"
      },
      "source": [
        "### TUTORIAL 4: REPLAY BUFFERS IN SEQUENTIAL TRAINING\n",
        "\n",
        "In this notebook, we will explore different replay buffer strategies. The two most common approaches are as follows:\n",
        "- Gnerative Replay buffer using VAEs- \"Store generated samples\"\n",
        "- Experience Replay- \"Store real samples from the dataset\"\n",
        "\n",
        "We will try to investigate how does storing real samples and generated samples impact the sequential training of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f6d9ad",
      "metadata": {
        "id": "d9f6d9ad"
      },
      "source": [
        "#### DATA Preparation and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf2c70b",
      "metadata": {
        "id": "9bf2c70b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from torchvision.models import resnet18\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "mean = torch.tensor([0.4589, 0.4384, 0.4011])\n",
        "std = torch.tensor([0.2793, 0.2724, 0.2835])\n",
        "\n",
        "# Apply transforms\n",
        "#transforms = transforms.Compose(\n",
        "#    [transforms.ToTensor()\n",
        "#    ])\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='/app/src/Mnist', train=True, download=True, transform= transforms.ToTensor())\n",
        "mnist_testset = datasets.MNIST(root='/app/src/Mnist', train=False, download=True, transform= transforms.ToTensor())\n",
        "\n",
        "len(mnist_trainset), len(mnist_testset) # 60000, 10000\n",
        "config = {'size': 28, 'channels': 1, 'classes': 10}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2f899a",
      "metadata": {
        "id": "fa2f899a"
      },
      "source": [
        "### Spliting dataset into N contexts\n",
        "\n",
        "- Let us first divide our dataset into five sequential tasks and we will do it based on the labels\n",
        "- Split with labels such that,  context 1: [0,1], context 2: [2,3],  context 1: [4,5], context 2: [6,7],  context 1: [8,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d383d94b",
      "metadata": {
        "id": "d383d94b"
      },
      "outputs": [],
      "source": [
        "train_data_cl = []\n",
        "test_data_cl = []\n",
        "labels = [[0,1], [2,3], [4,5], [6,7], [8,9]]\n",
        "labels_ = [[0,1], [0,1,2,3], [0,1,2,3,4,5], [0,1,2,3,4,5,6,7], [0,1,2,3,4,5,6,7,8,9]] ### For test sets, each contexts should have all the classes from the previous contexts\n",
        "for j in range(5):\n",
        "    train_data_cl.append([])\n",
        "    test_data_cl.append([])\n",
        "    for i in range(len(mnist_trainset)):\n",
        "        if mnist_trainset[i][1] in labels[j]:\n",
        "            train_data_cl[j].append((mnist_trainset[i][0], mnist_trainset[i][1]))\n",
        "    for i in range(len(mnist_testset)):\n",
        "        if mnist_testset[i][1] in labels_[j]:\n",
        "            test_data_cl[j].append((mnist_testset[i][0], mnist_testset[i][1]))\n",
        "\n",
        "\n",
        "#train_data_cl[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e87b86",
      "metadata": {
        "id": "17e87b86"
      },
      "source": [
        "#### Sequential training\n",
        "\n",
        "Let us now use this set of tasks to train a neural network sequentially, where we define a simple classifier or convolutational networks and train it on all the sequence of tasks  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eef2295",
      "metadata": {
        "id": "5eef2295"
      },
      "source": [
        "\"\" TO-DO \"\"\n",
        "- Define a simple MLP network or a Convolutional Network\n",
        "- Define your training loop\n",
        "- Train it on all the contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8bcd6e5",
      "metadata": {
        "id": "a8bcd6e5"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \"\"\" TODO\n",
        "        Include the rest of the layers here: Must be Linear layers and activation functions\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = nn.Flatten()(x)\n",
        "        \"\"\" TO-DO \"\"\"\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_classifier(model, train_data, test_data, num_epochs=10, batch_size=32, learning_rate=0.001):\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in tqdm(train_loader):\n",
        "            \"\"\" TO-DO \"\"\"\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Evaluation loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            \"\"\" TO-DO \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Accuracy of the model on the test data: {100 * correct / total:.2f}%\")\n",
        "\n",
        "\n",
        "### Define the model\n",
        "model = Classifier(input_dim=28*28, hidden_dim=256, output_dim=10)\n",
        "import os\n",
        "if not os.path.exists('/app/src/model_replay'):\n",
        "    os.makedirs('/app/src/model_replay')\n",
        "\n",
        "### Train the model on each context\n",
        "for i in range(5):\n",
        "\n",
        "    train_classifier(model, train_data_cl[i], test_data_cl[i], num_epochs=5, batch_size=32, learning_rate=0.1)\n",
        "    #torch.save(model.state_dict(), f'/app/src/model_replay/replay_{i}.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c599b727",
      "metadata": {
        "id": "c599b727"
      },
      "source": [
        "We could clearly observe the phenomenon of catastrophic forgetting here.\n",
        "We should also see the same phenomenon if we have to evaluate on individial contexts where the model performs the best on the last task and bad performance on the earlier tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a4504f",
      "metadata": {
        "id": "08a4504f"
      },
      "outputs": [],
      "source": [
        "### Evaluate the model on each context\n",
        "def evaluate_classifier(model, test_data, batch_size=32):\n",
        "    # Create data loader\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Evaluation loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            \"\"\" TO-DO \"\"\"\n",
        "\n",
        "\n",
        "    print(f\"Accuracy of the model on the test data: {100 * correct / total:.2f}%\")\n",
        "\n",
        "for i in range(5):\n",
        "    evaluate_classifier(model, test_data_cl[i], batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9667b9de",
      "metadata": {
        "id": "9667b9de"
      },
      "source": [
        "#### Accomodate Replay buffers\n",
        "\n",
        "We have two options\n",
        "- Store generative samples using VAEs\n",
        "- Store real samples as experience replay\n",
        "\n",
        "But, first, we need to define our VAE model, train it on all the contexts and observe its efficacy to generate new data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0097933",
      "metadata": {
        "id": "a0097933"
      },
      "source": [
        "### Generative Replay Buffers\n",
        "\n",
        "- We will use VAE to generate new samples, that you will store in the memory buffers\n",
        "- Define your VAE model\n",
        "- we will generate samples based on class instances [0], [1], etc\n",
        "- So, we will train our VAE model on samples from individual class instances. That means it is necessary to split the dataset based on the samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596cc15f",
      "metadata": {
        "id": "596cc15f"
      },
      "source": [
        "##### Split the dataset per labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "329490f3",
      "metadata": {
        "id": "329490f3"
      },
      "outputs": [],
      "source": [
        "train_data = []\n",
        "test_data = []\n",
        "#labels = [[0,1,2], [3,4,5], [6,7,8]]\n",
        "labels = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n",
        "\n",
        "#labels = [[0], [1], [2]]\n",
        "for j in range(10):\n",
        "    train_data.append([])\n",
        "    test_data.append([])\n",
        "    for i in range(len(mnist_trainset)):\n",
        "        if mnist_trainset[i][1] in labels[j]:\n",
        "            train_data[j].append((mnist_trainset[i][0], mnist_trainset[i][1]))\n",
        "    for i in range(len(mnist_testset)):\n",
        "        if mnist_testset[i][1] in labels[j]:\n",
        "            test_data[j].append((mnist_testset[i][0], mnist_testset[i][1]))\n",
        "\n",
        "\n",
        "train_data[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07e969f",
      "metadata": {
        "id": "b07e969f"
      },
      "source": [
        "##### Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e7562e1",
      "metadata": {
        "id": "9e7562e1"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        \"\"\" TO-DO \"\"\"\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = nn.Flatten()(x)\n",
        "        \"\"\" TO-DO \"\"\"\n",
        "\n",
        "        return mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        \"\"\" TO-DO \"\"\"\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\" TO-DO \"\"\"\n",
        "\n",
        "        return z\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, output_dim)\n",
        "        self.decoder = Decoder(output_dim, hidden_dim, input_dim)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\" TO-DO \"\"\"\n",
        "\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" TO-DO \"\"\"\n",
        "\n",
        "        return x_recon, mu, logvar\n",
        "    def loss_function(self, x, x_recon, mu, logvar):\n",
        "        \"\"\" TO-DO \"\"\"\n",
        "\n",
        "        return BCE + 5 * KLD\n",
        "\n",
        "    def sample(self, z):\n",
        "        \"\"\" TO-DO\n",
        "        1. Generate samples from the latent space\n",
        "        2. Use the decoder to reconstruct the images\n",
        "        3. Return the reconstructed images\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "\n",
        "            return z.view(-1, 1, 28, 28)\n",
        "    def generate(self, num_samples, output_dim):\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(num_samples, output_dim)\n",
        "            samples = self.decoder(z)\n",
        "            return samples.view(-1, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b562c871",
      "metadata": {
        "id": "b562c871"
      },
      "source": [
        "##### Train your VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68dc79a2",
      "metadata": {
        "id": "68dc79a2"
      },
      "outputs": [],
      "source": [
        "def train(train_data=None, test_data=None, t= None):\n",
        "    # Hyperparameters\n",
        "    input_dim = 28 * 28\n",
        "    hidden_dim = 400\n",
        "    output_dim = 20\n",
        "    batch_size = 64\n",
        "    num_epochs = 5\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    # Data loader\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model, loss function and optimizer\n",
        "    model = VAE(input_dim, hidden_dim, output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, _) in enumerate(tqdm(train_loader)):\n",
        "        \"\"\" TO-DO\n",
        "        1. Define the training loop\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset)}')\n",
        "\n",
        "        # Save the model checkpoint\n",
        "        torch.save(model.state_dict(), f'/app/src/vae_mnist_{t}.pth')\n",
        "    return model\n",
        "\n",
        "\n",
        "def test(model, test_data = None, batch_size= None, t= None):\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    # Load the model checkpoint\n",
        "    model.load_state_dict(torch.load(f'/app/src/vae_mnist_{t}.pth'))\n",
        "    for epoch in range(1):\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, _) in enumerate(tqdm(test_loader)):\n",
        "                data = data.view(-1, 28 * 28)\n",
        "                x_recon, mu, logvar = model(data)\n",
        "                loss = model.loss_function(data, x_recon, mu, logvar)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "        print(f'Test Loss: {test_loss / len(test_loader.dataset)}')\n",
        "\n",
        "        # Generate samples\n",
        "        samples = model.generate(64, output_dim=20)\n",
        "        grid = make_grid(samples, nrow=8)\n",
        "        plt.imshow(grid.permute(1, 2, 0).numpy())\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "input_dim = 28 * 28\n",
        "hidden_dim = 400\n",
        "output_dim = 2\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "for i in range(10):\n",
        "    model = train(train_data[i], test_data[i], t= i)\n",
        "    test(model, test_data[i], batch_size= 64, t= i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc12d4f",
      "metadata": {
        "id": "cdc12d4f"
      },
      "source": [
        "#### Populate Replay Buffer with your generated samples\n",
        "\n",
        "For each of these use cases, we will adopt reservoir sampling. We first fix the size of the memory buffer to N samples, for instance 10000. The idea is that at each time step, we should save equal number of samples for each of the previously seen class instances.\n",
        "\n",
        "If you recall, each of our contexts has only 2 class labels.\n",
        "So, at the end of the first task, we will store 5000 samples from labels [0] and [1].\n",
        "Similarily at the end of the second task, we will store 2500 samples from labels [0], [1], [2], [3]\n",
        "Continue it till the final context\n",
        "\n",
        "Replay buffer for each context should look this:\n",
        "- context_0: [5000, 5000]\n",
        "- context_1: [2500, 2500, 2500, 2500]\n",
        "- context_2:[1666, 1666, 1666, 1666, 1666, 1666]\n",
        "- context_3: [1250, 1250, 1250, 1250, 1250, 1250, 1250, 1250]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b7220df",
      "metadata": {
        "id": "2b7220df"
      },
      "outputs": [],
      "source": [
        "generator = VAE(28*28, 400, 20)\n",
        "mem_size = None\n",
        "contexts = 5\n",
        "replay = []\n",
        "for i in range(contexts - 1):\n",
        "\"\"\" TO-DO\n",
        "\n",
        "1. Generate samples from the pretrained VAE for each labels\n",
        "2. Store the generated samples in the replay buffer\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1620fb6e",
      "metadata": {
        "id": "1620fb6e"
      },
      "outputs": [],
      "source": [
        "model = Classifier(input_dim=28*28, hidden_dim=256, output_dim=10)\n",
        "for i in range(5):\n",
        "    if i == 0:\n",
        "        train_classifier(model, train_data_cl[i], test_data_cl[i], num_epochs=5, batch_size=64, learning_rate=0.1)\n",
        "    else:\n",
        "        train_classifier(model, train_data_cl[i]+ replay[i-1], test_data_cl[i], num_epochs=10, batch_size=64, learning_rate=0.1)\n",
        "    #torch.save(model.state_dict(), f'/app/src/model_replay/replay_{i}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c6338c",
      "metadata": {
        "id": "18c6338c"
      },
      "source": [
        "#### Experience Replay\n",
        "\n",
        "Now, we could perform the similar experiments with real data samples directly from the dataset. Use the same reservoir sampling approach to populate the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bccb522",
      "metadata": {
        "id": "1bccb522"
      },
      "outputs": [],
      "source": [
        "mem_size = 10000\n",
        "labels = [[[0],[1]], [[0],[1],[2],[3]], [[0],[1],[2],[3],[4],[5]], [[0],[1],[2],[3],[4],[5],[6],[7]], [[0],[1],[2],[3],[4],[5],[6], [7],[8],[9]]]\n",
        "\"\"\" TO_DO \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a6289a",
      "metadata": {
        "id": "90a6289a"
      },
      "source": [
        "#### Train all the contexts using replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42caa4b2",
      "metadata": {
        "id": "42caa4b2"
      },
      "outputs": [],
      "source": [
        "model = Classifier(input_dim=28*28, hidden_dim=256, output_dim=10)\n",
        "for i in range(5):\n",
        "\n",
        "    #model.load_state_dict(torch.load(f'/app/src/model_replay/replay_{i}.pth'))\n",
        "    #model = train_classifier(model, train_data[i], test_data[i], num_epochs=5, batch_size=32, learning_rate=0.001)\n",
        "    if i == 0:\n",
        "        train_classifier(model, train_data_cl[i], test_data_cl[i], num_epochs=5, batch_size=64, learning_rate=0.1)\n",
        "    else:\n",
        "        train_classifier(model, train_data_cl[i]+ replay[f'context_{i-1}'], test_data_cl[i], num_epochs=10, batch_size=64, learning_rate=0.1)\n",
        "    #torch.save(model.state_dict(), f'/app/src/model_replay/replay_{i}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c340bb",
      "metadata": {
        "id": "a3c340bb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}