# -*- coding: utf-8 -*-
"""Copy_of_Copy_of_Copy_of_MADE_Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/176_aJ1ihe8GHZbEvXPCXvfqIAAhZPiSZ

Mandatory imports
"""

import matplotlib.pyplot as plt
import numpy as np
import random
import tensorflow as tf
import time
import os
from keras import backend as k
from keras import metrics
from tensorflow.keras.datasets import mnist
from tensorflow.keras import activations
from tensorflow.keras.layers import Input, Layer
from tensorflow.keras.optimizers import Adam, Adagrad
from tensorflow.keras.datasets import mnist

def random_shuffle(seed, _list):
    random.seed(seed)
    random.shuffle(_list)

def np_save(base_dir, filename, data):
  if os.path.isdir(base_dir) == False:
      os.makedirs(base_dir)
  np.save(os.path.join(base_dir, filename), data)

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

X = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2]) #flatten 28x28 pixels to one dimension (784 inputs)
X = np.where(X > 127, 1, 0) #binarize x
test = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2]) #flatten 28x28 pixels to one dimension (784 inputs)
test = np.where(test > 127, 1, 0)
seed= 42
#shuffle dataset
length= len(X)-10000
print(length)
X_valid= X[length:]
x= X[:length]
num_clients= 3

base_dir = 'content'
for i in range(3):
  idx_shuffled = np.arange(len(x))
  random.seed(seed)
  random.shuffle(idx_shuffled)    
  data= x[idx_shuffled]
  idx = np.arange(len(X_valid))
  random.seed(seed)
  random.shuffle(idx_shuffled)
  val= X_valid[idx]  
  filename= f'Task_{i}'
  np_save(base_dir, filename, data)
  v_f = f'Val_{i}'
  np_save(base_dir, v_f, val)

"""Create Mask Generator Module for creating/managing MADEs masks"""

class MaskGenerator(object):
  # num_masks: The amount of masks that will be cycled through during training. if num_masks == 1 then connectivity agnostic training is disabled
  # units_per_layer = Array containing # of units per layer
  # seed = The seed used for randomly sampling the masks, for guaranteeing reproducability
  # natural_input_order = Boolean defining if the natural input order (x1, x2, x3 etc) should be used
  # current_mask: Integer to keep track of the mask currently used (xth mask)
  # m: The mask values assigned to the networks units. 0 is the index of the input layer, 1 is the index of the first hidden layer and so on
  def __init__(self, num_masks, units_per_layer, natural_input_order = False, seed=42):
    self.num_masks = num_masks
    self.units_per_layer = units_per_layer
    self.seed = seed
    self.natural_input_order = natural_input_order
    self.current_mask = 0
    self.m = {}

    if natural_input_order: # init input ordering according to settings
      self.m[0] = np.arange(self.units_per_layer[0])
    else:
      self.shuffle_inputs(return_mask = False)
  
  #Iterate through the hidden layers, resample new connectivity values m and build/return the resulting new masks
  def shuffle_masks(self):
    layer_amount = len(self.units_per_layer)
    rng = np.random.RandomState(self.seed+self.current_mask)
    self.current_mask = (self.current_mask + 1) % self.num_masks # Cycle through masks
    for i in range(1, layer_amount -1): #skip input layer & output layer and only iterate through hidden_layers
      self.m[i] = rng.randint(self.m[i-1].min(), self.units_per_layer[0] -1, size = self.units_per_layer[i]) # sample m from [min_m(previous_layer, d-1)] for all hidden units
    new_masks = [tf.convert_to_tensor((self.m[l-1][:, None] <= self.m[l][None,:]), dtype=np.float32) for l in range(1, layer_amount-1)] # build hidden layer masks
    new_masks.append(tf.convert_to_tensor((self.m[layer_amount-2][:, None] < self.m[0][None, :]), dtype = np.float32)) #build output layer mask. Note that the m values for the output layer are the same as for the input layer 
    return new_masks

  # builds & returns direct mask. Call this method after shuffling inputs if order_agnostic training is active.
  # Note that the Mask values m are the same for both input and output layers
  def get_direct_mask(self):
    return tf.convert_to_tensor((self.m[0][:, None] < self.m[0][None, :]), dtype = np.float32)

  # shuffle input ordering and return new mask for first hidden layer
  def shuffle_inputs(self, return_mask = True):
    self.m[0] = np.random.permutation(self.units_per_layer[0])
    if return_mask:
      return tf.convert_to_tensor((self.m[0][:, None] <= self.m[1][None,:]), dtype=np.float32)
    return

"""Custom Layer for MADE masking"""

# should be self explaining
class MaskedLayer(Layer):
    def __init__(self,
                units,
                mask,
                activation='relu',
                kernel_initializer='glorot_uniform',
                bias_initializer='zeros',
                **kwargs):
      self.units = units
      self.mask = mask
      self.activation = activations.get(activation)
      self.kernel_initializer = kernel_initializer
      self.bias_initializer = bias_initializer
      super(MaskedLayer, self).__init__(**kwargs)

    def build(self, input_shape):
      #self.input_dim = input_shape[-1] if self.x_dim is None else input_shape[0][-1]

      self.W = self.add_weight(shape=self.mask.shape,
                                  initializer=self.kernel_initializer,
                                  name='W')

      self.bias = self.add_weight(shape=(self.units,),
                                      initializer=self.bias_initializer,
                                      name='bias')

      self.built = True

    def call(self, inputs):
        ## Modified keras.Dense to account for the mask
        masked_weights = self.W*self.mask
        output = k.dot(inputs, masked_weights)
        output = k.bias_add(output, self.bias, data_format = 'channels_last')
        if self.activation is not None:
            output = self.activation(output)
        return output

    def set_mask(self, mask):
        self.mask = mask

    def get_mask(self):
        return self.mask

    def compute_output_shape(self, input_shape):
        ##Same as keras.Dense
        assert input_shape and len(input_shape) >= 2
        assert input_shape[-1]
        output_shape = list(input_shape)
        output_shape[-1] = self.units
        return tuple(output_shape)



class ConditionningMaskedLayer(MaskedLayer):
    def __init__(self, 
                units,
                mask,
                activation='relu',
                kernel_initializer='glorot_uniform',
                bias_initializer='zeros',
                use_cond_mask=False,
                **kwargs):
        self.use_cond_mask = use_cond_mask
        super(ConditionningMaskedLayer, self).__init__(units,
                mask,
                activation,
                kernel_initializer,
                bias_initializer, **kwargs)

    def build(self, input_shape):
        if self.use_cond_mask:
            self.U = self.add_weight(shape=self.mask.shape,
                                     initializer=self.kernel_initializer,
                                     name='U')
        super().build(input_shape)

    def call(self, inputs):
        if self.use_cond_mask == False:
          return super().call(inputs)
        masked_w_weights = self.W*self.mask
        masked_u_weights_times_one_vec = k.dot(tf.ones(tf.shape(inputs)),self.U*self.mask)
        weighted_input = k.dot(inputs, masked_w_weights)
        weighted_input_and_bias = k.bias_add(weighted_input, self.bias, data_format = 'channels_last')
        output = weighted_input_and_bias + masked_u_weights_times_one_vec
        if self.activation is not None:
            output = self.activation(output)
        return output



class DirectInputConnectConditionningMaskedLayer(ConditionningMaskedLayer):
      def __init__(self,
                   units,
                   mask,
                   activation='relu',
                   kernel_initializer='glorot_uniform',
                   bias_initializer='zeros',
                   use_cond_mask=False,
                   direct_mask = None,
                **kwargs):
        self.direct_mask = direct_mask
        super(DirectInputConnectConditionningMaskedLayer, self).__init__(units,
                mask,
                activation,
                kernel_initializer,
                bias_initializer,
                use_cond_mask,
                **kwargs)

      def build(self, input_shape):
        if self.direct_mask is not None:
          self.D = self.add_weight(shape=self.direct_mask.shape,
                                  initializer=self.kernel_initializer,
                                  name='D')
        super().build(input_shape)

      def set_mask(self, mask, direct = False):
        if direct:
          self.direct_mask = mask
        else:
          super().set_mask(mask)

      def get_mask(self, direct = False):
        if direct:
          return self.direct_mask
        else:
          return super().get_mask

      def call(self, inputs):
        if self.direct_mask is None:
          return super().call(inputs)
        input, direct_input = inputs[0], inputs[1]

        masked_w_weights = self.W*self.mask
        weighted_input = k.dot(input, masked_w_weights)
        weighted_input_and_bias = k.bias_add(weighted_input, self.bias, data_format = 'channels_last')
        weighted_direct_input = k.dot(direct_input, self.D * self.direct_mask)

        if self.use_cond_mask:
          masked_u_weights_times_one_vec = k.dot(tf.ones(tf.shape(input)),self.U*self.mask)
          output = weighted_direct_input + weighted_input_and_bias + masked_u_weights_times_one_vec

        else: output = weighted_direct_input + weighted_input_and_bias

        if self.activation is not None:
            output = self.activation(output)
        return output

"""# MADE Model"""

# outputs: output Layer   ---------- Both needed when using ----------
# inputs: input Layer     ----------    base keras.Model    ----------     
# mask_generator: Mask Generator instance that manages the Models Masks
# order_agn: Boolean defining if training should be order_agnostic
# conn_agn: Boolean defining if training should be connectivity_agnostic
# direct_input: Boolean defining if direct input masks should be used
class ModelMADE(tf.keras.Model):
    def __init__(self, inputs, outputs, mask_generator, order_agn, conn_agn,
                 direct_input, **kwargs):
      super(ModelMADE, self).__init__(inputs = inputs, outputs = outputs, **kwargs)
      self.mask_generator = mask_generator
      self.order_agn = order_agn
      self.conn_agn = conn_agn
      self.direct_input = direct_input
    
    # Method called by fit for every batch
    def train_step(self, data):

      # reoder inputs, change masks
      if self.order_agn:
        # order agnostic and connectivity agnostic training
        if self.conn_agn:
          self.mask_generator.shuffle_inputs(return_mask = False)
          new_masks = self.mask_generator.shuffle_masks()
          for hidden_layer_id in range(len(new_masks)):
            self.layers[1+hidden_layer_id].set_mask(new_masks[hidden_layer_id]) #assign layer+1 since the first layer is no hidden layer and has no mask
        
        # order agnostic but not connectivity agnostic training        
        else:
          self.layers[1].set_mask(self.mask_generator.shuffle_inputs())
        if self.direct_input:
          self.layers[-1].set_mask(self.mask_generator.get_direct_mask(), direct=True)

      # not order agnostic but connectivity agnostic training
      elif self.conn_agn:
        new_masks = self.mask_generator.shuffle_masks()
        for hidden_layer_id in range(len(new_masks)):
          self.layers[1+hidden_layer_id].set_mask(new_masks[hidden_layer_id])


      # Unpack the data. Its structure depends on your model and
      # on what you pass to `fit()`.
      x, y = data

      with tf.GradientTape() as tape:
        y_pred = self(x, training=True)  # Forward pass
        # Compute the loss value
        # (the loss function is configured in `compile()`)
        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)


      # Compute gradients
      trainable_vars = self.trainable_variables
      gradients = tape.gradient(loss, trainable_vars)
      # Update weights
      self.optimizer.apply_gradients(zip(gradients, trainable_vars))
      # Update metrics (includes the metric that tracks the loss)
      self.compiled_metrics.update_state(y, y_pred)
      # Return a dict mapping metric names to current value
      return {m.name: m.result() for m in self.metrics}

"""# MADE Object
responsible for building and inintalizing the MADE model
"""

# units_per_layer = Array containing # of units per layer
# natural_input_order = Boolean defining if the natural input order (x1, x2, x3 etc) should be used
# num_masks: The amount of masks that will be cycled through during training. if num_masks == 1 then connectivity agnostic training is disabled
# order_agn: Boolean defining if training should be order_agnostic
# connectivity_weights: Boolean defining if connectivity weights should be used
# direct input: Boolean defining if there should be a direct input connection between input & output layer
  # seed = The seed used for randomly sampling the masks, for guaranteeing reproducability
class MADE(object):
  def __init__(self, units_per_layer, natural_input_order, num_masks, order_agn,
               connectivity_weights, direct_input, seed = "42"):
    self.units_per_layer = units_per_layer
    self.natural_input_order = natural_input_order
    self.num_masks = num_masks
    self.order_agn = order_agn
    self.connectivity_weights = connectivity_weights
    self.direct_input = direct_input
    self.seed = seed
    self.mask_generator = MaskGenerator(num_masks, units_per_layer, natural_input_order, seed)

  def build_model(self):
    # build input layer
    a = Input(shape = (self.units_per_layer[0],))
    x_layers = []
      
    #build masks
    masks = self.mask_generator.shuffle_masks()
    direct_mask = None

    #build hidden layers  
    for i in range(1,len(self.units_per_layer)-1): #exclude input & output layer
      if i == 1:
        x_layers.append(ConditionningMaskedLayer(units = self.units_per_layer[i], mask = masks[i-1], use_cond_mask = self.connectivity_weights)(a)) #activation is relu, call custom_masking with previous layer as input-param
      else:
        x_layers.append(ConditionningMaskedLayer(units = self.units_per_layer[i], mask = masks[i-1], use_cond_mask = self.connectivity_weights)(x_layers[i-1]))
          
    #build output layer, output layer's activation is sigmoid.
    if self.direct_input:
      direct_mask = self.mask_generator.get_direct_mask()
      output_layer = DirectInputConnectConditionningMaskedLayer(units = self.units_per_layer[-1], mask = masks[-1], activation='sigmoid', use_cond_mask = self.connectivity_weights, direct_mask = direct_mask)([x_layers[-1], a])
    else:
      output_layer = ConditionningMaskedLayer(units = self.units_per_layer[-1], mask = masks[-1], activation='sigmoid', use_cond_mask = self.connectivity_weights)(x_layers[-1])
    x_layers.append(output_layer)
    
    self.model = ModelMADE(inputs = a, outputs = x_layers[-1], mask_generator = self.mask_generator, order_agn = self.order_agn, conn_agn = self.num_masks>1,
                           direct_input=self.direct_input)
    return self.model

  def summary(self):
    return self.model.summary()

"""# Loss Function"""

def cross_entropy_loss(x, x_decoded_mean):
    x = k.flatten(x)
    x_decoded_mean = k.flatten(x_decoded_mean)
    xent_loss = 150 * metrics.binary_crossentropy(x, x_decoded_mean)
    #print("shape", len(X_train[1]))
    return xent_loss

def average_model(client_weights):
  av_param= []
  avg= 1/len(client_weights)
  #avg= 0.1
  print(avg)
  for i in range(len(client_weights[0])):
    av_param.append([])
  for i in range(len(client_weights)):
    for j in range(len(client_weights[i])):
      if i ==0:
        av_param[j].append(avg * client_weights[i][j])
      else:
        av_param[j] = av_param[j] + (avg * client_weights[i][j])
  for i in range(len(client_weights[0])):
    av_param[i]=np.squeeze(av_param[i])
  return av_param

"""# Build & Run Model"""

######################### Settings #########################
_optimizer_type = "adam" #for any other string here then adam Adagrad is used
_adam_lr = 0.001 #0.1, 0.05, 0.01, 0.005
_ada_lr = 0.001 #0.1, 0.05, 0.01, 0.005
_ada_epsilon = 1e-6

_hidden_layers = [110]
_natural_input_order = False
_num_masks = 2
_order_agn = True
_order_agn_step_size = 1
_conn_agn_step_size = 1
_connectivity_weights = True
_direct_input = True
_seed = 777
_batch_size = 100
_epochs = 1

if _optimizer_type == "adam": 
  optimizer = Adam(_adam_lr)
else: 
  optimizer = Adagrad(_ada_lr, epsilon = _ada_epsilon)

tf.keras.backend.clear_session()

units_per_layer = np.concatenate(([150], _hidden_layers, [150])) #in MADE case the input & output layer have the same amount of units
print("shape",units_per_layer)
seeds= [777]

print(seeds)
data_split= ['sull']#['full', 12000, 6000, 3000, 1500]
clients= [1] #[3,5,10,20,40]
for i in range(len(data_split)):
  print(f" START TRAINING DATA SPLIT {data_split[i]}")
  seed = seeds[i]
  num_clients= clients[i]

    


  for _seed in seeds:
    temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, 
                connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed)
    model = temp.build_model()
    model.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)

    start = time.time()
    print("enter")
    #num_clients = 2
    num_rounds= 20
    num_tasks= 4

    loss = {}
    for c in range(num_clients):
      loss[f'{c}']= []

    val_loss = {}
    for c in range(num_clients):
      val_loss[f'{c}']= []
    for t in range(num_tasks):
      for r in range(num_rounds):   
        client_weights= []
        for c in range(num_clients):
          #X= np.load(f'/home/subarna/Pictures/LARS/FedWeIT-MADE/output/binary/binary_{t*num_clients+ c}_train.npy', allow_pickle=True) #, allow_pickle=True) FedWeIT-MADE/
          #X_val= np.load(f'/home/subarna/Pictures/LARS/FedWeIT-MADE/output/binary/binary_{t*num_clients+ c}_valid.npy', allow_pickle=True) #, allow_pickle=True)
          data= np.load(f'output/binary/binary_{t*num_clients+ c}_train.npy', allow_pickle=True).item()
          X=  tf.convert_to_tensor(data['x_train'])
          val_data= np.load(f'output/binary/binary_{t*num_clients+ c}_valid.npy', allow_pickle=True).item()
          X_valid= val_data['x_valid']
          #X=  tf.convert_to_tensor(data['x_train'])
          history = model.fit(
              X, X,
              batch_size=_batch_size,
              epochs=_epochs,
              validation_data=(X_valid, X_valid)
          )  
          
          #print(history.history.keys())
          #val_loss[f'{c}'].append(history.history['val_loss'])
          loss[f'{c}'].append(history.history['loss'])
          temp= history.history['loss']
          el= model.get_weights()
          #print(el)
          if t > 0:
            for pt in range(t):
              val_data= np.load(f'/home/subarna/Pictures/LARS/FedWeIT-MADE/output/binary/binary_{(pt)*num_clients+ c}_valid.npy', allow_pickle=True).item()
              val= val_data['x_valid']
              test_loss=model.evaluate(val, val, batch_size=_batch_size)
              f = open("/home/subarna/Pictures/LARS/FedWeIT-MADE/binary_experiment_new_fedmade.txt", "a")
              f.write('\n'+f'forgetting loss of task  {pt} for client {c} at current task {t}: {test_loss}' )
              f.close()

            

          client_weights.append(el)
          f = open("/home/subarna/Pictures/LARS/FedWeIT-MADE/binary_experiment_new_fedmade.txt", "a")
          f.write('\n'+
                  f'[task {t}]'+
                  f'[round {r}]'+
                  f'[client {c}] '+
                  f'[loss:{temp}]')
          f.close()
        #model_param= average_model(client_weights)
        #model.set_weights(model_param)

    done = time.time()
    elapsed = done - start
    print("Elapsed: ", elapsed)
    print(f"Number of masks: {_num_masks}")
    test_loss=model.evaluate(X_test, X_test, batch_size=_batch_size)
    #t_loss.append(test_loss)
    print(f"Test Loss: {test_loss}")

    #np.save(f'content/VAL_Loss_CM_{_seed}', val_loss)
    #np.save(f'/app/src/FedWit/output/Loss_CM_{_seed}_{data_split[i]}', loss)
  #np.save(f'content/Test_loss_{data_split[i]}', test_loss)
  #weights= model.get_weights()
  #np.save(f'content/modelweights_{data_split[i]}', weights)
  #print(_seed)

#model.save('/content/mymodel')
#model= keras.models.load_model('/content/mymodel')

