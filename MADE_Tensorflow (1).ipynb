{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QJANkmenWsaF"
      },
      "source": [
        "Mandatory imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-IjSfHlnWl6h"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-28 11:08:41.473377: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-04-28 11:08:41.473422: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from keras import backend as k\n",
        "from keras import metrics\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.layers import Input, Layer\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VWllsz-OWzcC"
      },
      "source": [
        "Load Dataset ( https://github.com/mgermain/MADE/releases/download/ICML2015/binarized_mnist.npz )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJxod15qW0Mz",
        "outputId": "8c8adb92-8b9d-45cc-e565-4e57d59f788b"
      },
      "outputs": [],
      "source": [
        "# Example for loading the data\n",
        "#!wget https://github.com/mgermain/MADE/releases/download/ICML2015/binarized_mnist.npz \n",
        "# I stored the dataset in my drive\n",
        "#def load_from_drive():\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "x_temp = X_train\n",
        "x = x_temp.reshape(x_temp.shape[0],x_temp.shape[1]*x_temp.shape[2]) #flatten 28x28 pixels to one dimension (784 inputs)\n",
        "x = np.where(x > 127, 1, 0) #binarize x\n",
        "y = Y_train\n",
        "seed= 77\n",
        "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2]) #flatten 28x28 pixels to one dimension (784 inputs)\n",
        "X_test = np.where(X_test > 127, 1, 0) \n",
        "\n",
        "#load_from_drive()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N1N06S0jXBMe"
      },
      "source": [
        "Create Mask Generator Module for creating/managing MADEs masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DmT3Yp76XBtp"
      },
      "outputs": [],
      "source": [
        "class MaskGenerator(object):\n",
        "  # num_masks: The amount of masks that will be cycled through during training. if num_masks == 1 then connectivity agnostic training is disabled\n",
        "  # units_per_layer = Array containing # of units per layer\n",
        "  # seed = The seed used for randomly sampling the masks, for guaranteeing reproducability\n",
        "  # natural_input_order = Boolean defining if the natural input order (x1, x2, x3 etc) should be used\n",
        "  # current_mask: Integer to keep track of the mask currently used (xth mask)\n",
        "  # m: The mask values assigned to the networks units. 0 is the index of the input layer, 1 is the index of the first hidden layer and so on\n",
        "  def __init__(self, num_masks, units_per_layer, natural_input_order = False, seed=42):\n",
        "    self.num_masks = num_masks\n",
        "    self.units_per_layer = units_per_layer\n",
        "    self.seed = seed\n",
        "    self.natural_input_order = natural_input_order\n",
        "    self.current_mask = 0\n",
        "    self.m = {}\n",
        "\n",
        "    if natural_input_order: # init input ordering according to settings\n",
        "      self.m[0] = np.arange(self.units_per_layer[0])\n",
        "    else:\n",
        "      self.shuffle_inputs(return_mask = False)\n",
        "  \n",
        "  #Iterate through the hidden layers, resample new connectivity values m and build/return the resulting new masks\n",
        "  def shuffle_masks(self):\n",
        "    layer_amount = len(self.units_per_layer)\n",
        "    rng = np.random.RandomState(self.seed+self.current_mask)\n",
        "    self.current_mask = (self.current_mask + 1) % self.num_masks # Cycle through masks\n",
        "    for i in range(1, layer_amount -1): #skip input layer & output layer and only iterate through hidden_layers\n",
        "      self.m[i] = rng.randint(self.m[i-1].min(), self.units_per_layer[0] -1, size = self.units_per_layer[i]) # sample m from [min_m(previous_layer, d-1)] for all hidden units\n",
        "    new_masks = [tf.convert_to_tensor((self.m[l-1][:, None] <= self.m[l][None,:]), dtype=np.float32) for l in range(1, layer_amount-1)] # build hidden layer masks\n",
        "    new_masks.append(tf.convert_to_tensor((self.m[layer_amount-2][:, None] < self.m[0][None, :]), dtype = np.float32)) #build output layer mask. Note that the m values for the output layer are the same as for the input layer \n",
        "    return new_masks\n",
        "\n",
        "  # builds & returns direct mask. Call this method after shuffling inputs if order_agnostic training is active.\n",
        "  # Note that the Mask values m are the same for both input and output layers\n",
        "  def get_direct_mask(self):\n",
        "    return tf.convert_to_tensor((self.m[0][:, None] < self.m[0][None, :]), dtype = np.float32)\n",
        "\n",
        "  # shuffle input ordering and return new mask for first hidden layer\n",
        "  def shuffle_inputs(self, return_mask = True):\n",
        "    self.m[0] = np.random.permutation(self.units_per_layer[0])\n",
        "    if return_mask:\n",
        "      return tf.convert_to_tensor((self.m[0][:, None] <= self.m[1][None,:]), dtype=np.float32)\n",
        "    return"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk-sv9C9XDtt"
      },
      "source": [
        "Custom Layer for MADE masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lm1c8achXFmr"
      },
      "outputs": [],
      "source": [
        "# should be self explaining\n",
        "class MaskedLayer(Layer):\n",
        "    def __init__(self,\n",
        "                units,\n",
        "                mask,\n",
        "                activation='relu',\n",
        "                kernel_initializer='glorot_uniform',\n",
        "                bias_initializer='zeros',\n",
        "                **kwargs):\n",
        "      self.units = units\n",
        "      self.mask = mask\n",
        "      self.activation = activations.get(activation)\n",
        "      self.kernel_initializer = kernel_initializer\n",
        "      self.bias_initializer = bias_initializer\n",
        "      super(MaskedLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      #self.input_dim = input_shape[-1] if self.x_dim is None else input_shape[0][-1]\n",
        "\n",
        "      self.W = self.add_weight(shape=self.mask.shape,\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  name='W')\n",
        "\n",
        "      self.bias = self.add_weight(shape=(self.units,),\n",
        "                                      initializer=self.bias_initializer,\n",
        "                                      name='bias')\n",
        "\n",
        "      self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        ## Modified keras.Dense to account for the mask\n",
        "        masked_weights = self.W*self.mask\n",
        "        output = k.dot(inputs, masked_weights)\n",
        "        output = k.bias_add(output, self.bias, data_format = 'channels_last')\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def set_mask(self, mask):\n",
        "        self.mask = mask\n",
        "\n",
        "    def get_mask(self):\n",
        "        return self.mask\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ##Same as keras.Dense\n",
        "        assert input_shape and len(input_shape) >= 2\n",
        "        assert input_shape[-1]\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] = self.units\n",
        "        return tuple(output_shape)\n",
        "\n",
        "\n",
        "\n",
        "class ConditionningMaskedLayer(MaskedLayer):\n",
        "    def __init__(self, \n",
        "                units,\n",
        "                mask,\n",
        "                activation='relu',\n",
        "                kernel_initializer='glorot_uniform',\n",
        "                bias_initializer='zeros',\n",
        "                use_cond_mask=False,\n",
        "                **kwargs):\n",
        "        self.use_cond_mask = use_cond_mask\n",
        "        super(ConditionningMaskedLayer, self).__init__(units,\n",
        "                mask,\n",
        "                activation,\n",
        "                kernel_initializer,\n",
        "                bias_initializer, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.use_cond_mask:\n",
        "            self.U = self.add_weight(shape=self.mask.shape,\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='U')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.use_cond_mask == False:\n",
        "          return super().call(inputs)\n",
        "        masked_w_weights = self.W*self.mask\n",
        "        masked_u_weights_times_one_vec = k.dot(tf.ones(tf.shape(inputs)),self.U*self.mask)\n",
        "        weighted_input = k.dot(inputs, masked_w_weights)\n",
        "        weighted_input_and_bias = k.bias_add(weighted_input, self.bias, data_format = 'channels_last')\n",
        "        output = weighted_input_and_bias + masked_u_weights_times_one_vec\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class DirectInputConnectConditionningMaskedLayer(ConditionningMaskedLayer):\n",
        "      def __init__(self,\n",
        "                   units,\n",
        "                   mask,\n",
        "                   activation='relu',\n",
        "                   kernel_initializer='glorot_uniform',\n",
        "                   bias_initializer='zeros',\n",
        "                   use_cond_mask=False,\n",
        "                   direct_mask = None,\n",
        "                **kwargs):\n",
        "        self.direct_mask = direct_mask\n",
        "        super(DirectInputConnectConditionningMaskedLayer, self).__init__(units,\n",
        "                mask,\n",
        "                activation,\n",
        "                kernel_initializer,\n",
        "                bias_initializer,\n",
        "                use_cond_mask,\n",
        "                **kwargs)\n",
        "\n",
        "      def build(self, input_shape):\n",
        "        if self.direct_mask is not None:\n",
        "          self.D = self.add_weight(shape=self.direct_mask.shape,\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  name='D')\n",
        "        super().build(input_shape)\n",
        "\n",
        "      def set_mask(self, mask, direct = False):\n",
        "        if direct:\n",
        "          self.direct_mask = mask\n",
        "        else:\n",
        "          super().set_mask(mask)\n",
        "\n",
        "      def get_mask(self, direct = False):\n",
        "        if direct:\n",
        "          return self.direct_mask\n",
        "        else:\n",
        "          return super().get_mask\n",
        "\n",
        "      def call(self, inputs):\n",
        "        if self.direct_mask is None:\n",
        "          return super().call(inputs)\n",
        "        input, direct_input = inputs[0], inputs[1]\n",
        "\n",
        "        masked_w_weights = self.W*self.mask\n",
        "        weighted_input = k.dot(input, masked_w_weights)\n",
        "        weighted_input_and_bias = k.bias_add(weighted_input, self.bias, data_format = 'channels_last')\n",
        "        weighted_direct_input = k.dot(direct_input, self.D * self.direct_mask)\n",
        "\n",
        "        if self.use_cond_mask:\n",
        "          masked_u_weights_times_one_vec = k.dot(tf.ones(tf.shape(input)),self.U*self.mask)\n",
        "          output = weighted_direct_input + weighted_input_and_bias + masked_u_weights_times_one_vec\n",
        "\n",
        "        else: output = weighted_direct_input + weighted_input_and_bias\n",
        "\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kj3sQmCCXHjj"
      },
      "source": [
        "# MADE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "leKoxMM1XJtn"
      },
      "outputs": [],
      "source": [
        "# outputs: output Layer   ---------- Both needed when using ----------\n",
        "# inputs: input Layer     ----------    base keras.Model    ----------     \n",
        "# mask_generator: Mask Generator instance that manages the Models Masks\n",
        "# order_agn: Boolean defining if training should be order_agnostic\n",
        "# conn_agn: Boolean defining if training should be connectivity_agnostic\n",
        "# direct_input: Boolean defining if direct input masks should be used\n",
        "class ModelMADE(tf.keras.Model):\n",
        "    def __init__(self, inputs, outputs, mask_generator, order_agn, conn_agn,\n",
        "                 direct_input, **kwargs):\n",
        "      super(ModelMADE, self).__init__(inputs = inputs, outputs = outputs, **kwargs)\n",
        "      self.mask_generator = mask_generator\n",
        "      self.order_agn = order_agn\n",
        "      self.conn_agn = conn_agn\n",
        "      self.direct_input = direct_input\n",
        "    \n",
        "    # Method called by fit for every batch\n",
        "    def train_step(self, data):\n",
        "\n",
        "      # reoder inputs, change masks\n",
        "      if self.order_agn:\n",
        "        # order agnostic and connectivity agnostic training\n",
        "        if self.conn_agn:\n",
        "          self.mask_generator.shuffle_inputs(return_mask = False)\n",
        "          new_masks = self.mask_generator.shuffle_masks()\n",
        "          for hidden_layer_id in range(len(new_masks)):\n",
        "            self.layers[1+hidden_layer_id].set_mask(new_masks[hidden_layer_id]) #assign layer+1 since the first layer is no hidden layer and has no mask\n",
        "        \n",
        "        # order agnostic but not connectivity agnostic training        \n",
        "        else:\n",
        "          self.layers[1].set_mask(self.mask_generator.shuffle_inputs())\n",
        "        if self.direct_input:\n",
        "          self.layers[-1].set_mask(self.mask_generator.get_direct_mask(), direct=True)\n",
        "\n",
        "      # not order agnostic but connectivity agnostic training\n",
        "      elif self.conn_agn:\n",
        "        new_masks = self.mask_generator.shuffle_masks()\n",
        "        for hidden_layer_id in range(len(new_masks)):\n",
        "          self.layers[1+hidden_layer_id].set_mask(new_masks[hidden_layer_id])\n",
        "\n",
        "\n",
        "      # Unpack the data. Its structure depends on your model and\n",
        "      # on what you pass to `fit()`.\n",
        "      x, y = data\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_pred = self(x, training=True)  # Forward pass\n",
        "        # Compute the loss value\n",
        "        # (the loss function is configured in `compile()`)\n",
        "        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "\n",
        "      # Compute gradients\n",
        "      trainable_vars = self.trainable_variables\n",
        "      gradients = tape.gradient(loss, trainable_vars)\n",
        "      # Update weights\n",
        "      self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "      # Update metrics (includes the metric that tracks the loss)\n",
        "      self.compiled_metrics.update_state(y, y_pred)\n",
        "      # Return a dict mapping metric names to current value\n",
        "      return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "af_jZIw7XLfA"
      },
      "source": [
        "# MADE Object\n",
        "responsible for building and inintalizing the MADE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6v-oRKzRXNnf"
      },
      "outputs": [],
      "source": [
        "# units_per_layer = Array containing # of units per layer\n",
        "# natural_input_order = Boolean defining if the natural input order (x1, x2, x3 etc) should be used\n",
        "# num_masks: The amount of masks that will be cycled through during training. if num_masks == 1 then connectivity agnostic training is disabled\n",
        "# order_agn: Boolean defining if training should be order_agnostic\n",
        "# connectivity_weights: Boolean defining if connectivity weights should be used\n",
        "# direct input: Boolean defining if there should be a direct input connection between input & output layer\n",
        "  # seed = The seed used for randomly sampling the masks, for guaranteeing reproducability\n",
        "class MADE(object):\n",
        "  def __init__(self, units_per_layer, natural_input_order, num_masks, order_agn,\n",
        "               connectivity_weights, direct_input, seed = \"42\"):\n",
        "    self.units_per_layer = units_per_layer\n",
        "    self.natural_input_order = natural_input_order\n",
        "    self.num_masks = num_masks\n",
        "    self.order_agn = order_agn\n",
        "    self.connectivity_weights = connectivity_weights\n",
        "    self.direct_input = direct_input\n",
        "    self.seed = seed\n",
        "    self.mask_generator = MaskGenerator(num_masks, units_per_layer, natural_input_order, seed)\n",
        "\n",
        "  def build_model(self):\n",
        "    # build input layer\n",
        "    a = Input(shape = (self.units_per_layer[0],))\n",
        "    x_layers = []\n",
        "      \n",
        "    #build masks\n",
        "    masks = self.mask_generator.shuffle_masks()\n",
        "    direct_mask = None\n",
        "\n",
        "    #build hidden layers  \n",
        "    for i in range(1,len(self.units_per_layer)-1): #exclude input & output layer\n",
        "      if i == 1:\n",
        "        x_layers.append(ConditionningMaskedLayer(units = self.units_per_layer[i], mask = masks[i-1], use_cond_mask = self.connectivity_weights)(a)) #activation is relu, call custom_masking with previous layer as input-param\n",
        "      else:\n",
        "        x_layers.append(ConditionningMaskedLayer(units = self.units_per_layer[i], mask = masks[i-1], use_cond_mask = self.connectivity_weights)(x_layers[i-1]))\n",
        "          \n",
        "    #build output layer, output layer's activation is sigmoid.\n",
        "    if self.direct_input:\n",
        "      direct_mask = self.mask_generator.get_direct_mask()\n",
        "      output_layer = DirectInputConnectConditionningMaskedLayer(units = self.units_per_layer[-1], mask = masks[-1], activation='sigmoid', use_cond_mask = self.connectivity_weights, direct_mask = direct_mask)([x_layers[-1], a])\n",
        "    else:\n",
        "      output_layer = ConditionningMaskedLayer(units = self.units_per_layer[-1], mask = masks[-1], activation='sigmoid', use_cond_mask = self.connectivity_weights)(x_layers[-1])\n",
        "    x_layers.append(output_layer)\n",
        "    \n",
        "    self.model = ModelMADE(inputs = a, outputs = x_layers[-1], mask_generator = self.mask_generator, order_agn = self.order_agn, conn_agn = self.num_masks>1,\n",
        "                           direct_input=self.direct_input)\n",
        "    return self.model\n",
        "\n",
        "  def summary(self):\n",
        "    return self.model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot0YX5q0XRGP"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4XpIHE6eXRw2"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(x, x_decoded_mean):\n",
        "    x = k.flatten(x)\n",
        "    x_decoded_mean = k.flatten(x_decoded_mean)\n",
        "    #print(\"loss\", len(X_train[1]))\n",
        "    xent_loss = 150 * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "    return xent_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_model(client_weights):\n",
        "  av_param= []\n",
        "  avg= 1/len(client_weights)\n",
        "  #avg= 0.1\n",
        "  print(avg)\n",
        "  for i in range(len(client_weights[0])):\n",
        "    av_param.append([])\n",
        "  for i in range(len(client_weights)):\n",
        "    for j in range(len(client_weights[i])):\n",
        "      if i ==0:\n",
        "        av_param[j].append(avg * client_weights[i][j])\n",
        "      else:\n",
        "        av_param[j] = av_param[j] + (avg * client_weights[i][j])\n",
        "  for i in range(len(client_weights[0])):\n",
        "    av_param[i]=np.squeeze(av_param[i])\n",
        "  return av_param"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqgBcy8XU3U"
      },
      "source": [
        "# Build & Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "######################### Settings #########################\n",
        "_optimizer_type = \"ada\" #for any other string here then adam Adagrad is used\n",
        "_adam_lr = 0.001 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_lr = 0.001 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_epsilon = 1e-6\n",
        "\n",
        "_hidden_layers = [500]\n",
        "_natural_input_order = False\n",
        "_num_masks = 1\n",
        "_order_agn = True\n",
        "_order_agn_step_size = 1\n",
        "_conn_agn_step_size = 1\n",
        "_connectivity_weights = False\n",
        "_direct_input = True\n",
        "_seed = 42\n",
        "_batch_size = 100\n",
        "_epochs = 1\n",
        "\n",
        "if _optimizer_type == \"adam\": \n",
        "  optimizer = Adam(_adam_lr)\n",
        "else: \n",
        "  optimizer = Adagrad(_ada_lr, epsilon = _ada_epsilon)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "units_per_layer = np.concatenate(([784], _hidden_layers, [784])) #in MADE case the input & output layer have the same amount of units\n",
        "print(\"shape\",units_per_layer)\n",
        "seeds= [42]\n",
        "\n",
        "print(seeds)\n",
        "data_split= ['sull']#['full', 12000, 6000, 3000, 1500]\n",
        "clients= [3] #[3,5,10,20,40]\n",
        "for i in range(len(data_split)):\n",
        "  print(f\" START TRAINING DATA SPLIT {data_split[i]}\")\n",
        "  seed = seeds[i]\n",
        "  num_clients= clients[i]\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  for _seed in seeds:\n",
        "    temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "                connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed)\n",
        "    model = temp.build_model()\n",
        "    model.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "\n",
        "    start = time.time()\n",
        "    print(\"enter\")\n",
        "    #num_clients = 2\n",
        "    num_rounds= 50\n",
        "    num_tasks= 1\n",
        "\n",
        "    loss = {}\n",
        "    for c in range(num_clients):\n",
        "      loss[f'{c}']= []\n",
        "\n",
        "    val_loss = {}\n",
        "    for c in range(num_clients):\n",
        "      val_loss[f'{c}']= []\n",
        "    for t in range(num_tasks):\n",
        "      for r in range(num_rounds):   \n",
        "        client_weights= []\n",
        "        for c in range(num_clients):\n",
        "          X= np.load(f'/home/subarna/Pictures/LARS/FedWeIT-MADE/content/Task_{t*num_clients+ c}.npy')\n",
        "          X_val= np.load(f'/home/subarna/Pictures/LARS/FedWeIT-MADE/content/Val_{t*num_clients+ c}.npy')\n",
        "\n",
        "          #X=  tf.convert_to_tensor(data['x_train'])\n",
        "          history = model.fit(\n",
        "              X, X,\n",
        "              batch_size=_batch_size,\n",
        "              epochs=_epochs,\n",
        "              validation_data=(X_val, X_val)\n",
        "          )\n",
        "          \n",
        "          #print(history.history.keys())\n",
        "          #val_loss[f'{c}'].append(history.history['val_loss'])\n",
        "          loss[f'{c}'].append(history.history['loss'])\n",
        "          temp= history.history['loss']\n",
        "          el= model.get_weights()\n",
        "          #print(el)\n",
        "          if t > 0:\n",
        "            for pt in range(t):\n",
        "              val_data= np.load(f'/home/subarna/Pictures/LARS/FedWeIT-MADE/output/binary/binary_{(pt)*num_clients+ c}_valid.npy')\n",
        "              val= val_data['x_valid']\n",
        "              test_loss=model.evaluate(val, val, batch_size=_batch_size)\n",
        "              f = open(\"/home/subarna/Pictures/LARS/FedWeIT-MADE/federated_offline.txt\", \"a\")\n",
        "              f.write('\\n'+f'forgetting loss of task  {pt} for client {c} at current task {t}: {test_loss}' )\n",
        "              f.close()\n",
        "\n",
        "            \n",
        "\n",
        "          client_weights.append(el)\n",
        "          f = open(\"/home/subarna/Pictures/LARS/FedWeIT-MADE/federated_offline.txt\", \"a\")\n",
        "          f.write('\\n'+\n",
        "                  f'[task {t}]'+\n",
        "                  f'[round {r}]'+\n",
        "                  f'[client {c}] '+\n",
        "                  f'[loss:{temp}]')\n",
        "          f.close()\n",
        "        model_param= average_model(client_weights)\n",
        "        model.set_weights(model_param)\n",
        "\n",
        "    done = time.time()\n",
        "    elapsed = done - start\n",
        "    print(\"Elapsed: \", elapsed)\n",
        "    print(f\"Number of masks: {_num_masks}\")\n",
        "    test_loss=model.evaluate(X_test, X_test, batch_size=_batch_size)\n",
        "    #t_loss.append(test_loss)\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "    #np.save(f'content/VAL_Loss_CM_{_seed}', val_loss)\n",
        "    #np.save(f'/app/src/FedWit/output/Loss_CM_{_seed}_{data_split[i]}', loss)\n",
        "  #np.save(f'content/Test_loss_{data_split[i]}', test_loss)\n",
        "  #weights= model.get_weights()\n",
        "  #np.save(f'content/modelweights_{data_split[i]}', weights)\n",
        "  #print(_seed)\n",
        "\n",
        "#model.save('/content/mymodel')\n",
        "#model= keras.models.load_model('/content/mymodel')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eq3iJXffXVcG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_made\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150)]             0         \n",
            "                                                                 \n",
            " conditionning_masked_layer   (None, 500)              75500     \n",
            " (ConditionningMaskedLayer)                                      \n",
            "                                                                 \n",
            " conditionning_masked_layer_  (None, 150)              75150     \n",
            " 1 (ConditionningMaskedLayer                                     \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 150,650\n",
            "Trainable params: 150,650\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-28 11:35:33.376768: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2023-04-28 11:35:33.376792: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2023-04-28 11:35:33.376810: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (subarna-ThinkPad-P14s-Gen-2a): /proc/driver/nvidia/version does not exist\n",
            "2023-04-28 11:35:33.377018: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1268/1268 [==============================] - 13s 11ms/step - loss: 40.3145 - val_loss: 75.5076\n",
            "Epoch 2/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 29.1780 - val_loss: 81.6740\n",
            "Epoch 3/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 26.8116 - val_loss: 81.2059\n",
            "Epoch 4/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 25.4810 - val_loss: 80.7568\n",
            "Epoch 5/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 24.5644 - val_loss: 81.4268\n",
            "Epoch 6/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 23.9364 - val_loss: 88.5635\n",
            "Epoch 7/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 23.3362 - val_loss: 85.8890\n",
            "Epoch 8/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 22.9659 - val_loss: 87.3143\n",
            "Epoch 9/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 22.6750 - val_loss: 84.1473\n",
            "Epoch 10/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 22.3728 - val_loss: 85.4079\n",
            "Epoch 11/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 22.1852 - val_loss: 89.6852\n",
            "Epoch 12/50\n",
            "1268/1268 [==============================] - 14s 11ms/step - loss: 21.8588 - val_loss: 82.6609\n",
            "Epoch 13/50\n",
            "1268/1268 [==============================] - 15s 11ms/step - loss: 21.8155 - val_loss: 84.7317\n",
            "Epoch 14/50\n",
            "1268/1268 [==============================] - 16s 12ms/step - loss: 21.5686 - val_loss: 86.9397\n",
            "Epoch 15/50\n",
            "1268/1268 [==============================] - 17s 13ms/step - loss: 21.4664 - val_loss: 87.2773\n",
            "Epoch 16/50\n",
            "1268/1268 [==============================] - 17s 14ms/step - loss: 21.3387 - val_loss: 94.5396\n",
            "Epoch 17/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 21.1951 - val_loss: 87.2029\n",
            "Epoch 18/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 21.0586 - val_loss: 86.8682\n",
            "Epoch 19/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 21.0703 - val_loss: 91.8687\n",
            "Epoch 20/50\n",
            "1268/1268 [==============================] - 19s 15ms/step - loss: 20.8528 - val_loss: 87.8491\n",
            "Epoch 21/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 20.8153 - val_loss: 91.7439\n",
            "Epoch 22/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 20.7322 - val_loss: 81.8832\n",
            "Epoch 23/50\n",
            "1268/1268 [==============================] - 17s 14ms/step - loss: 20.6306 - val_loss: 87.0790\n",
            "Epoch 24/50\n",
            "1268/1268 [==============================] - 17s 14ms/step - loss: 20.6245 - val_loss: 96.7661\n",
            "Epoch 25/50\n",
            "1268/1268 [==============================] - 17s 13ms/step - loss: 20.5576 - val_loss: 93.2469\n",
            "Epoch 26/50\n",
            "1268/1268 [==============================] - 17s 13ms/step - loss: 20.4497 - val_loss: 93.3448\n",
            "Epoch 27/50\n",
            "1268/1268 [==============================] - 17s 13ms/step - loss: 20.4183 - val_loss: 83.1005\n",
            "Epoch 28/50\n",
            "1268/1268 [==============================] - 17s 13ms/step - loss: 20.3789 - val_loss: 96.1024\n",
            "Epoch 29/50\n",
            "1268/1268 [==============================] - 17s 13ms/step - loss: 20.3897 - val_loss: 88.9929\n",
            "Epoch 30/50\n",
            "1268/1268 [==============================] - 17s 13ms/step - loss: 20.2831 - val_loss: 88.3225\n",
            "Epoch 31/50\n",
            "1268/1268 [==============================] - 17s 14ms/step - loss: 20.1868 - val_loss: 90.7298\n",
            "Epoch 32/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 20.2188 - val_loss: 96.0225\n",
            "Epoch 33/50\n",
            "1268/1268 [==============================] - 17s 13ms/step - loss: 20.1702 - val_loss: 91.6180\n",
            "Epoch 34/50\n",
            "1268/1268 [==============================] - 17s 14ms/step - loss: 20.0831 - val_loss: 91.5686\n",
            "Epoch 35/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 20.0400 - val_loss: 90.3008\n",
            "Epoch 36/50\n",
            "1268/1268 [==============================] - 17s 14ms/step - loss: 20.0885 - val_loss: 89.8737\n",
            "Epoch 37/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 20.0111 - val_loss: 86.5271\n",
            "Epoch 38/50\n",
            "1268/1268 [==============================] - 18s 15ms/step - loss: 19.9639 - val_loss: 98.6704\n",
            "Epoch 39/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 19.9655 - val_loss: 99.6184\n",
            "Epoch 40/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 19.8936 - val_loss: 82.8768\n",
            "Epoch 41/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 19.8677 - val_loss: 87.6682\n",
            "Epoch 42/50\n",
            "1268/1268 [==============================] - 20s 16ms/step - loss: 19.8021 - val_loss: 91.9791\n",
            "Epoch 43/50\n",
            "1268/1268 [==============================] - 18s 14ms/step - loss: 19.8260 - val_loss: 96.8551\n",
            "Epoch 44/50\n",
            "1268/1268 [==============================] - 16s 13ms/step - loss: 19.7804 - val_loss: 89.5807\n",
            "Epoch 45/50\n",
            "1268/1268 [==============================] - 15s 12ms/step - loss: 19.7348 - val_loss: 98.7681\n",
            "Epoch 46/50\n",
            "1268/1268 [==============================] - 22s 17ms/step - loss: 19.7619 - val_loss: 81.8232\n",
            "Epoch 47/50\n",
            "1268/1268 [==============================] - 24s 19ms/step - loss: 19.7448 - val_loss: 90.8836\n",
            "Epoch 48/50\n",
            "1268/1268 [==============================] - 23s 19ms/step - loss: 19.6721 - val_loss: 83.1918\n",
            "Epoch 49/50\n",
            "1268/1268 [==============================] - 22s 18ms/step - loss: 19.6441 - val_loss: 87.2398\n",
            "Epoch 50/50\n",
            "1268/1268 [==============================] - 20s 16ms/step - loss: 19.6450 - val_loss: 93.0735\n",
            "Elapsed:  853.6430950164795\n",
            "Number of masks: 1\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/subarna/Pictures/LARS/FedWeIT-MADE/MADE_Tensorflow (1).ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/subarna/Pictures/LARS/FedWeIT-MADE/MADE_Tensorflow%20%281%29.ipynb#X23sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mElapsed: \u001b[39m\u001b[39m\"\u001b[39m, elapsed)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/subarna/Pictures/LARS/FedWeIT-MADE/MADE_Tensorflow%20%281%29.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of masks: \u001b[39m\u001b[39m{\u001b[39;00m_num_masks\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/subarna/Pictures/LARS/FedWeIT-MADE/MADE_Tensorflow%20%281%29.ipynb#X23sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m test_loss\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mevaluate(X_test, X_test, batch_size\u001b[39m=\u001b[39m_batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/subarna/Pictures/LARS/FedWeIT-MADE/MADE_Tensorflow%20%281%29.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ],
      "source": [
        "######################### Settings #########################\n",
        "_optimizer_type = \"ada\" #for any other string here then adam Adagrad is used\n",
        "_adam_lr = 0.001 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_lr = 0.005 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_epsilon = 1e-6\n",
        "\n",
        "_hidden_layers = [500]\n",
        "_natural_input_order = False,\n",
        "_num_masks = 1\n",
        "_order_agn = True\n",
        "_order_agn_step_size = 1\n",
        "_conn_agn_step_size = 1\n",
        "_connectivity_weights = False\n",
        "_direct_input = False\n",
        "_seed = 42\n",
        "_batch_size = 100\n",
        "_epochs = 50\n",
        "\n",
        "if _optimizer_type == \"adam\": \n",
        "  optimizer = Adam(_adam_lr)\n",
        "else: \n",
        "  optimizer = Adagrad(_ada_lr, epsilon = _ada_epsilon)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "#print(\"build\",X_train.shape)\n",
        "units_per_layer = np.concatenate(([150], _hidden_layers, [150])) #in MADE case the input & output layer have the same amount of units\n",
        "\n",
        "temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "            connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed)\n",
        "model = temp.build_model()\n",
        "model.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "model.summary()\n",
        "\n",
        "start = time.time()\n",
        "data= np.load(f'output/binary/binary_0_train.npy', allow_pickle=True).item()\n",
        "X=  tf.convert_to_tensor(data['x_train'])\n",
        "val_data= np.load(f'output/binary/binary_0_valid.npy', allow_pickle=True).item()\n",
        "X_valid= val_data['x_valid']\n",
        "#test_data= np.load(f'output/binary/binary_0_test.npy', allow_pickle=True).item()\n",
        "#X_test= test_data['x_test']\n",
        "plt.history = model.fit(\n",
        "              X, X,\n",
        "              batch_size=_batch_size,\n",
        "              epochs=_epochs,\n",
        "              validation_data=(X_valid, X_valid)\n",
        "                       ) \n",
        "done = time.time()\n",
        "elapsed = done - start\n",
        "print(\"Elapsed: \", elapsed)\n",
        "print(f\"Number of masks: {_num_masks}\")\n",
        "test_loss=model.evaluate(X_test, X_test, batch_size=_batch_size)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: results/mod/assets\n"
          ]
        }
      ],
      "source": [
        "model.save('results/mod')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from emnist import extract_training_samples\n",
        "import numpy as np\n",
        "images, label = extract_training_samples('letters')\n",
        "images= np.where(images> 127, 1,0)\n",
        "x_emnist = images.reshape(images.shape[0],images.shape[1]*images.shape[2])\n",
        "num= [1,2,3,4,5,6,7,8,9,10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "48000"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx= np.concatenate([np.where(label[:] == c)[0] for c in num], axis= 0)\n",
        "len(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "data= images[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(48000, 28, 28)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "MADE Tensorflow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
